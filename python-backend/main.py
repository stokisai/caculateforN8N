from fastapi import FastAPI, UploadFile, File, HTTPException, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
import pandas as pd
import io
import zipfile
import os
from typing import Optional, List, Dict, Tuple
from supabase import create_client, Client
import json
import locale
import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import asyncio
import aiohttp
from datetime import datetime
import uuid
import base64

# âœ… H10 å¤„ç†æ¨¡å—
try:
    from h10_processor import process_h10_analysis
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªå ä½ç¬¦å‡½æ•°
    async def process_h10_analysis(*args, **kwargs):
        raise HTTPException(status_code=500, detail="H10 å¤„ç†æ¨¡å—æœªæ­£ç¡®å¯¼å…¥")

# --- é…ç½®éƒ¨åˆ† ---
app = FastAPI(title="Excel Processing API", version="1.0.0")

# RapidAPI é…ç½®
RAPIDAPI_KEY = "a28a42a0a7mshddbb4f5e053ac79p10bb74jsn713f074877fc"
RAPIDAPI_HOST = "realtime-amazon-data.p.rapidapi.com"  # æ³¨æ„ï¼šä½ æä¾›çš„ host æ˜¯ realtimeï¼ˆæ²¡æœ‰è¿å­—ç¬¦ï¼‰

# å…è®¸è·¨åŸŸè¯·æ±‚ï¼ˆè¿™æ ·ä½ çš„å‰ç«¯æ‰èƒ½è°ƒè¿™ä¸ªæ¥å£ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒå»ºè®®æŠŠ * æ¢æˆä½ å‰ç«¯çš„åŸŸåï¼Œå¦‚ ["https://yourdomain.com"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¿æ¥ Supabaseï¼ˆå¯é€‰ï¼Œç”¨äºä»»åŠ¡æŒä¹…åŒ–å­˜å‚¨ï¼‰
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")  # å»ºè®®ä½¿ç”¨ service_role key

supabase: Optional[Client] = None
if SUPABASE_URL and SUPABASE_KEY:
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("âœ… Supabase è¿æ¥æˆåŠŸï¼ˆä»»åŠ¡å°†æŒä¹…åŒ–å­˜å‚¨ï¼‰")
    except Exception as e:
        print(f"âš ï¸ Supabase è¿æ¥å¤±è´¥: {e}")
        print("   æç¤º: ä»»åŠ¡å°†å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œé‡å¯åä¼šä¸¢å¤±")
else:
    # âœ… ä¿®å¤ï¼šSupabase æ˜¯å¯é€‰çš„ï¼Œä¸æ˜¾ç¤ºè­¦å‘Šï¼ˆä»»åŠ¡å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼‰
    print("â„¹ï¸ æç¤º: Supabase æœªé…ç½®ï¼Œä»»åŠ¡å°†å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼ˆé‡å¯åä¼šä¸¢å¤±ï¼‰")
    print("   å¦‚éœ€æŒä¹…åŒ–å­˜å‚¨ï¼Œè¯·åœ¨ Railway ç¯å¢ƒå˜é‡ä¸­è®¾ç½® SUPABASE_URL å’Œ SUPABASE_KEY")

# ç¤¾åª’é€‰å“æ³•æœåŠ¡é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
# âš ï¸ æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒåº”è¯¥é€šè¿‡ Railway ç¯å¢ƒå˜é‡è®¾ç½®ï¼Œä¸è¦ç¡¬ç¼–ç å¯†é’¥
SERP_API_KEY = os.getenv("SERP_API_KEY", "081c24883966800829defaacc9226d81832f54fbeb82b82bda1f5c8a9d01df40")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "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")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
SERP_API_URL = "https://serpapi.com/search"
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"
REFERENCE_IMAGE_URL = "https://m.media-amazon.com/images/I/61HVDJy8R4L._SL1500_.jpg"

# âœ… ä¿®å¤ï¼šæ£€æµ‹å ä½ç¬¦APIå¯†é’¥
def is_placeholder_key(key: str) -> bool:
    """æ£€æµ‹æ˜¯å¦æ˜¯å ä½ç¬¦å¯†é’¥"""
    if not key:
        return False
    placeholder_patterns = [
        "__n8n_BLANK_VALUE_",
        "__BLANK_VALUE__",
        "your-",
        "placeholder",
        "example",
        "test-key",
        "REPLACE_ME"
    ]
    key_lower = key.lower()
    return any(pattern.lower() in key_lower for pattern in placeholder_patterns)

# æ£€æŸ¥ API å¯†é’¥é…ç½®
if not OPENROUTER_API_KEY:
    print("âŒ é”™è¯¯: OPENROUTER_API_KEY æœªè®¾ç½®ï¼LLM ç”Ÿæˆå°†å¤±è´¥ã€‚")
    print("   è¯·åœ¨ Railway ç¯å¢ƒå˜é‡ä¸­è®¾ç½® OPENROUTER_API_KEY")
elif is_placeholder_key(OPENROUTER_API_KEY):
    print(f"âŒ é”™è¯¯: OPENROUTER_API_KEY æ˜¯å ä½ç¬¦å€¼ï¼å½“å‰å€¼: {OPENROUTER_API_KEY[:50]}...")
    print("   è¯·æ›¿æ¢ä¸ºçœŸå®çš„ OpenRouter API å¯†é’¥")
    print("   è·å–å¯†é’¥: https://openrouter.ai/keys")
    OPENROUTER_API_KEY = ""  # æ¸…ç©ºå ä½ç¬¦ï¼Œé¿å…ä½¿ç”¨

if not SERP_API_KEY:
    print("âŒ é”™è¯¯: SERP_API_KEY æœªè®¾ç½®ï¼SERP æœç´¢å°†å¤±è´¥ã€‚")
    print("   è¯·åœ¨ Railway ç¯å¢ƒå˜é‡ä¸­è®¾ç½® SERP_API_KEY")
elif is_placeholder_key(SERP_API_KEY):
    print(f"âŒ é”™è¯¯: SERP_API_KEY æ˜¯å ä½ç¬¦å€¼ï¼å½“å‰å€¼: {SERP_API_KEY[:50]}...")
    print("   è¯·æ›¿æ¢ä¸ºçœŸå®çš„ SerpAPI å¯†é’¥")
    print("   è·å–å¯†é’¥: https://serpapi.com/dashboard")
    SERP_API_KEY = ""  # æ¸…ç©ºå ä½ç¬¦ï¼Œé¿å…ä½¿ç”¨

# ä»»åŠ¡å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis æˆ–æ•°æ®åº“ï¼‰
job_storage: Dict[str, Dict] = {}

# --- æ ¸å¿ƒé€»è¾‘ ---
@app.post("/process")
async def process_excel(
    file: Optional[UploadFile] = File(None),
    service_id: Optional[str] = Form(None),
    input_text: Optional[str] = Form(None),
    # âœ… H10 æœåŠ¡å¤šæ–‡ä»¶ä¸Šä¼ 
    file_H10åæŸ¥æ€»è¡¨: Optional[UploadFile] = File(None),
    file_ç«å“1: Optional[UploadFile] = File(None),
    file_ç«å“2: Optional[UploadFile] = File(None),
    file_ç«å“3: Optional[UploadFile] = File(None),
    file_ç«å“4: Optional[UploadFile] = File(None),
    file_ç«å“5: Optional[UploadFile] = File(None),
    file_ç«å“6: Optional[UploadFile] = File(None),
    file_ç«å“7: Optional[UploadFile] = File(None),
    file_ç«å“8: Optional[UploadFile] = File(None),
    file_ç«å“9: Optional[UploadFile] = File(None),
    file_ç«å“10: Optional[UploadFile] = File(None),
    file_è‡ªèº«ASINåæŸ¥: Optional[UploadFile] = File(None),
    file_ç«å¯¹ABAçƒ­æœè¯åæŸ¥: Optional[UploadFile] = File(None),
    file_æ‹“è¯åŸºç¡€è¡¨: Optional[UploadFile] = File(None)
):
    """
    å¤„ç†ä¸Šä¼ çš„ Excel/ZIP æ–‡ä»¶æˆ–æ–‡æœ¬è¾“å…¥
    
    å‚æ•°:
    - file: ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒ .xlsx, .xls, .zipï¼‰ï¼Œå¯¹äºæ–‡æœ¬è¾“å…¥æœåŠ¡å¯ä¸ºç©º
    - service_id: å¯é€‰çš„æœåŠ¡IDï¼ˆç”¨äºè®°å½•ï¼‰
    - input_text: å¯é€‰çš„æ–‡æœ¬è¾“å…¥ï¼ˆå¯¹äºæ–‡æœ¬è¾“å…¥æœåŠ¡å¿…éœ€ï¼‰
    """
    # æ£€æŸ¥æœåŠ¡ç±»å‹ï¼šå¦‚æœæ˜¯"ç¤¾åª’é€‰å“æ³•"ï¼Œåªéœ€è¦ input_textï¼Œä¸éœ€è¦ file
    if service_id == "7b83cf63-0ad0-4c11-8dc5-6d8c242fbfe6":
        if not input_text or not input_text.strip():
            raise HTTPException(status_code=400, detail="ç¤¾åª’é€‰å“æ³•æœåŠ¡éœ€è¦æä¾›å…³é”®è¯ï¼ˆinput_textï¼‰")
        
        keyword = input_text.strip()
        
        # âœ… ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒå…³é”®è¯çš„ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼ˆé˜²æ­¢é‡å¤æäº¤ï¼‰
        running_jobs = [
            (job_id, job) 
            for job_id, job in job_storage.items() 
            if job.get("keyword") == keyword and job.get("status") in ["queued", "running"]
        ]
        
        if running_jobs:
            existing_job_id, existing_job = running_jobs[0]
            print(f"âš ï¸ è­¦å‘Š: å…³é”®è¯ '{keyword}' çš„ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­ (Job ID: {existing_job_id}, çŠ¶æ€: {existing_job.get('status')})")
            return JSONResponse({
                "message": f"å…³é”®è¯ '{keyword}' çš„ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­ï¼ŒJob ID: {existing_job_id}ã€‚è¯·ç­‰å¾…å®Œæˆæˆ–ä½¿ç”¨è¯¥ Job ID æŸ¥è¯¢è¿›åº¦ã€‚",
                "job_id": existing_job_id,
                "warning": "duplicate_keyword"
            }, status_code=200)  # è¿”å› 200 è€Œä¸æ˜¯é”™è¯¯ï¼Œå› ä¸ºä»»åŠ¡ç¡®å®å­˜åœ¨
        
        # ç›´æ¥è°ƒç”¨å¼‚æ­¥ä»»åŠ¡å¤„ç†é€»è¾‘
        job_id = str(uuid.uuid4())
        print(f"âœ… åˆ›å»ºæ–°ä»»åŠ¡: Job ID={job_id}, å…³é”®è¯={keyword}")
        job_storage[job_id] = {
            "status": "queued",
            "keyword": keyword,
            "progress": 0.0,
            "sections": [],
            "created_at": datetime.now().isoformat(),
            "artifacts": {}
        }
        # å¯åŠ¨åå°ä»»åŠ¡
        asyncio.create_task(execute_research_job(job_id, keyword))
        return JSONResponse({
            "message": f"ä»»åŠ¡å·²åˆ›å»ºï¼ŒJob ID: {job_id}ã€‚è¯·ä½¿ç”¨ GET /api/jobs/{job_id} æŸ¥è¯¢è¿›åº¦ã€‚",
            "job_id": job_id
        })
    
    # âœ… H10ç«å“åˆ†ææœåŠ¡ï¼šæ£€æŸ¥ service_id
    H10_SERVICE_ID = "a8f3c2d1-4e5b-6c7d-8e9f-0a1b2c3d4e5f"
    
    # âœ… H10 æœåŠ¡ç‰¹æ®Šå¤„ç†
    if service_id == H10_SERVICE_ID:
        try:
            result_stream = await process_h10_analysis(
                file_H10åæŸ¥æ€»è¡¨=file_H10åæŸ¥æ€»è¡¨,
                file_ç«å“1=file_ç«å“1,
                file_ç«å“2=file_ç«å“2,
                file_ç«å“3=file_ç«å“3,
                file_ç«å“4=file_ç«å“4,
                file_ç«å“5=file_ç«å“5,
                file_ç«å“6=file_ç«å“6,
                file_ç«å“7=file_ç«å“7,
                file_ç«å“8=file_ç«å“8,
                file_ç«å“9=file_ç«å“9,
                file_ç«å“10=file_ç«å“10,
                file_è‡ªèº«ASINåæŸ¥=file_è‡ªèº«ASINåæŸ¥,
                file_ç«å¯¹ABAçƒ­æœè¯åæŸ¥=file_ç«å¯¹ABAçƒ­æœè¯åæŸ¥,
                file_æ‹“è¯åŸºç¡€è¡¨=file_æ‹“è¯åŸºç¡€è¡¨,
                folder_files=None  # æ–‡ä»¶å¤¹æ–‡ä»¶é€šè¿‡å•ç‹¬çš„æ–‡ä»¶å‚æ•°ä¼ é€’
            )
            
            # è¿”å›å¤„ç†åçš„ Excel æ–‡ä»¶
            # âœ… ä¿®å¤ï¼šæ­£ç¡®å¤„ç†åŒ…å«ä¸­æ–‡å­—ç¬¦çš„æ–‡ä»¶åï¼ˆä½¿ç”¨ RFC 5987 æ ¼å¼ï¼‰
            result_filename = "H10åæŸ¥æ€»è¡¨_processed.xlsx"
            # ä½¿ç”¨ RFC 5987 æ ¼å¼æ”¯æŒ UTF-8 ç¼–ç çš„æ–‡ä»¶å
            from urllib.parse import quote as url_quote  # ç¡®ä¿ quote å¯ç”¨
            try:
                # å°è¯•ä½¿ç”¨ ASCII ç¼–ç 
                result_filename.encode('ascii')
                # æ–‡ä»¶ååªåŒ…å« ASCII å­—ç¬¦ï¼Œç›´æ¥ä½¿ç”¨
                content_disposition = f'attachment; filename="{result_filename}"'
            except UnicodeEncodeError:
                # åŒ…å«é ASCII å­—ç¬¦ï¼Œä½¿ç”¨ RFC 5987 æ ¼å¼
                # ç”Ÿæˆä¸€ä¸ª ASCII å®‰å…¨çš„ fallback æ–‡ä»¶å
                safe_ascii_filename = "H10_result.xlsx"
                # URL ç¼–ç åŸå§‹æ–‡ä»¶åç”¨äº UTF-8 ç‰ˆæœ¬
                encoded_filename = url_quote(result_filename, safe='')
                # ä½¿ç”¨ RFC 5987 æ ¼å¼ï¼šfilename æ˜¯ ASCII fallbackï¼Œfilename* æ˜¯ UTF-8 ç‰ˆæœ¬
                content_disposition = f'attachment; filename="{safe_ascii_filename}"; filename*=UTF-8\'\'{encoded_filename}'
            
            return StreamingResponse(
                result_stream,
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={
                    "Content-Disposition": content_disposition
                }
            )
        except HTTPException:
            raise
        except Exception as e:
            print(f"âŒ H10 å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"H10 å¤„ç†å¤±è´¥: {str(e)}")
    
    # å¯¹äºéœ€è¦æ–‡ä»¶çš„æœåŠ¡ï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if service_id != "7b83cf63-0ad0-4c11-8dc5-6d8c242fbfe6":  # ç¤¾åª’é€‰å“æ³•ä¸éœ€è¦æ–‡ä»¶
        if not file:
            raise HTTPException(status_code=400, detail="æ­¤æœåŠ¡éœ€è¦ä¸Šä¼ æ–‡ä»¶")
    
    # âœ… ä¿®å¤ï¼šå®‰å…¨å¤„ç†æ–‡ä»¶åï¼ˆå¯èƒ½åŒ…å«é ASCII å­—ç¬¦ï¼‰
    original_filename = file.filename or "uploaded_file" if file else "uploaded_file"
    try:
        # å°è¯•è§£ç æ–‡ä»¶åï¼ˆå¦‚æœæ˜¯ä» HTTP header æ¥çš„ï¼Œå¯èƒ½æ˜¯ URL ç¼–ç çš„ï¼‰
        if isinstance(original_filename, bytes):
            original_filename = original_filename.decode('utf-8', errors='replace')
    except:
        pass
    
    print(f"ğŸ“¥ æ”¶åˆ°æ–‡ä»¶: {original_filename}, ç±»å‹: {file.content_type}")
    print(f"ğŸ”‘ Service ID: {service_id}")
    print(f"ğŸ“ Input Text: {input_text}")
    
    try:
        # 1. è¯»å–ä¸Šä¼ çš„æ–‡ä»¶
        content = await file.read()
        file_extension = os.path.splitext(original_filename)[1].lower()
        
        df = None
        result_filename = None
        
        # 2. æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
        if file_extension == '.zip':
            # å¤„ç† ZIP æ–‡ä»¶
            if not zipfile.is_zipfile(io.BytesIO(content)):
                raise HTTPException(status_code=400, detail="ä¸Šä¼ çš„ä¸æ˜¯æœ‰æ•ˆçš„ ZIP æ–‡ä»¶")
            
            input_zip = zipfile.ZipFile(io.BytesIO(content))
            file_list = input_zip.namelist()
            
            # âœ… H10ç«å“åˆ†ææœåŠ¡ï¼šå¤„ç† ZIP ä¸­çš„æ‰€æœ‰ Excel æ–‡ä»¶
            if service_id == H10_SERVICE_ID:
                print(f"ğŸ“¦ H10ç«å“åˆ†æï¼šå¤„ç† ZIP æ–‡ä»¶ï¼ŒåŒ…å« {len(file_list)} ä¸ªæ–‡ä»¶")
                # æŸ¥æ‰¾æ‰€æœ‰ Excel æ–‡ä»¶
                excel_files = [f for f in file_list if f.endswith((".xlsx", ".xls"))]
                if not excel_files:
                    raise HTTPException(status_code=400, detail=f"å‹ç¼©åŒ…é‡Œæ²¡æ‰¾åˆ° Excel æ–‡ä»¶ã€‚æ–‡ä»¶åˆ—è¡¨: {file_list}")
                
                print(f"ğŸ“Š æ‰¾åˆ° {len(excel_files)} ä¸ª Excel æ–‡ä»¶: {excel_files}")
                # TODO: ç­‰å¾…ç”¨æˆ·æä¾›å…·ä½“å¤„ç†é€»è¾‘
                # æš‚æ—¶è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹ï¼Œåç»­éœ€è¦å¤„ç†æ‰€æœ‰æ–‡ä»¶
                with input_zip.open(excel_files[0]) as f:
                    df = pd.read_excel(f)
                result_filename = f"h10_analysis_{int(pd.Timestamp.now().timestamp() * 1000)}.xlsx"
            else:
                # å…¶ä»–æœåŠ¡ï¼šå¯»æ‰¾ç›®æ ‡æ–‡ä»¶ï¼ˆå¯ä»¥æ ¹æ® service_id æˆ–æ–‡ä»¶åæ¨¡å¼åŒ¹é…ï¼‰
                target_file = None
                if service_id == "h10" or "h10" in original_filename.lower():
                    # æŸ¥æ‰¾åŒ…å« H10 çš„ Excel æ–‡ä»¶
                    target_file = next((f for f in file_list if "H10" in f.upper() and (f.endswith(".xlsx") or f.endswith(".xls"))), None)
                else:
                    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª Excel æ–‡ä»¶
                    target_file = next((f for f in file_list if f.endswith((".xlsx", ".xls"))), None)
                
                if not target_file:
                    raise HTTPException(status_code=400, detail=f"å‹ç¼©åŒ…é‡Œæ²¡æ‰¾åˆ° Excel æ–‡ä»¶ã€‚æ–‡ä»¶åˆ—è¡¨: {file_list}")
                
                print(f"ğŸ“‚ æ‰¾åˆ°ç›®æ ‡æ–‡ä»¶: {target_file}")
                
                # è¯»å– Excel
                with input_zip.open(target_file) as f:
                    df = pd.read_excel(f)
                
                result_filename = f"processed_{os.path.splitext(target_file)[0]}.xlsx"
            
        elif file_extension in ['.xlsx', '.xls']:
            # ç›´æ¥å¤„ç† Excel æ–‡ä»¶
            # âœ… H10ç«å“åˆ†ææœåŠ¡ï¼šæ”¯æŒå•ä¸ª Excel æ–‡ä»¶
            if service_id == H10_SERVICE_ID:
                print(f"ğŸ“Š H10ç«å“åˆ†æï¼šå¤„ç†å•ä¸ª Excel æ–‡ä»¶: {original_filename}")
                df = pd.read_excel(io.BytesIO(content))
                result_filename = f"h10_analysis_{int(pd.Timestamp.now().timestamp() * 1000)}.xlsx"
            else:
                df = pd.read_excel(io.BytesIO(content))
                # âœ… ä¿®å¤ï¼šå®‰å…¨å¤„ç†æ–‡ä»¶åï¼Œé¿å…ç¼–ç é—®é¢˜
                base_name = os.path.splitext(original_filename)[0]
                # å¦‚æœæ–‡ä»¶ååŒ…å«é ASCII å­—ç¬¦ï¼Œä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å
                try:
                    base_name.encode('ascii')
                    result_filename = f"processed_{base_name}.xlsx"
                except UnicodeEncodeError:
                    # åŒ…å«é ASCII å­—ç¬¦ï¼Œä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºæ–‡ä»¶å
                    result_filename = f"processed_result_{int(pd.Timestamp.now().timestamp() * 1000)}.xlsx"
        else:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}ã€‚æ”¯æŒ: .xlsx, .xls, .zip")
        
        if df is None or df.empty:
            raise HTTPException(status_code=400, detail="è¯»å–çš„ Excel æ–‡ä»¶ä¸ºç©º")
        
        
        # 3. ä¸šåŠ¡é€»è¾‘å¤„ç†ï¼ˆæ ¹æ® service_id æ‰§è¡Œä¸åŒçš„å¤„ç†ï¼‰
        result = process_dataframe(df, service_id, input_text)
        
        # 4. æ£€æŸ¥è¿”å›ç±»å‹ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ˆæ–‡æœ¬æŠ¥å‘Šï¼‰ï¼Œè¿”å›çº¯æ–‡æœ¬æ–‡ä»¶ï¼›å¦åˆ™è¿”å› Excel
        if isinstance(result, str):
            # è¿”å›çº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆ.txtï¼‰
            text_bytes = result.encode('utf-8')
            return StreamingResponse(
                io.BytesIO(text_bytes),
                media_type="text/plain; charset=utf-8",
                headers={
                    "Content-Disposition": f'attachment; filename="roi_report_{int(pd.Timestamp.now().timestamp() * 1000)}.txt"'
                }
            )
        
        # 5. å¯¼å‡ºç»“æœåˆ°å†…å­˜ï¼ˆExcel æ–‡ä»¶ï¼‰
        output = io.BytesIO()
        result.to_excel(output, index=False, engine='openpyxl')
        output.seek(0)
        
        # 6. è¿”å›æ–‡ä»¶æµï¼ˆç›´æ¥ä¸‹è½½ï¼Œä¸ç»è¿‡ Supabaseï¼‰
        # âœ… ä¿®å¤ï¼šå¤„ç†ä¸­æ–‡æ–‡ä»¶åç¼–ç é—®é¢˜
        # ä½¿ç”¨ RFC 5987 æ ¼å¼æ”¯æŒ UTF-8 ç¼–ç çš„æ–‡ä»¶å
        from urllib.parse import quote
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆå¦‚æœåŒ…å«é ASCII å­—ç¬¦ï¼Œä½¿ç”¨ URL ç¼–ç ï¼‰
        try:
            # å°è¯•ä½¿ç”¨ ASCII ç¼–ç 
            result_filename.encode('ascii')
            # æ–‡ä»¶ååªåŒ…å« ASCII å­—ç¬¦ï¼Œç›´æ¥ä½¿ç”¨
            content_disposition = f'attachment; filename="{result_filename}"'
        except UnicodeEncodeError:
            # åŒ…å«é ASCII å­—ç¬¦ï¼Œä½¿ç”¨ RFC 5987 æ ¼å¼
            # ç”Ÿæˆä¸€ä¸ª ASCII å®‰å…¨çš„ fallback æ–‡ä»¶å
            safe_ascii_filename = f"result_{int(pd.Timestamp.now().timestamp() * 1000)}.xlsx"
            # URL ç¼–ç åŸå§‹æ–‡ä»¶åç”¨äº UTF-8 ç‰ˆæœ¬
            encoded_filename = quote(result_filename, safe='')
            # ä½¿ç”¨ RFC 5987 æ ¼å¼ï¼šfilename æ˜¯ ASCII fallbackï¼Œfilename* æ˜¯ UTF-8 ç‰ˆæœ¬
            content_disposition = f'attachment; filename="{safe_ascii_filename}"; filename*=UTF-8\'\'{encoded_filename}'
        
        return StreamingResponse(
            io.BytesIO(output.getvalue()),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": content_disposition
            }
        )
        
    except HTTPException:
        raise
    except ValueError as e:
        # ValueError é€šå¸¸æ˜¯ä¸šåŠ¡é€»è¾‘é”™è¯¯ï¼ˆå¦‚æ‰¾ä¸åˆ°åˆ—ï¼‰ï¼Œè¿”å› 400
        print(f"âŒ ä¸šåŠ¡é€»è¾‘é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥: {str(e)}")


def process_dataframe(df: pd.DataFrame, service_id: Optional[str], input_text: Optional[str]):
    """
    æ ¹æ®æœåŠ¡IDæ‰§è¡Œä¸åŒçš„æ•°æ®å¤„ç†é€»è¾‘
    
    è¿”å›:
    - å¦‚æœæ˜¯æ–‡æœ¬æŠ¥å‘ŠæœåŠ¡ï¼Œè¿”å› str
    - å¦‚æœæ˜¯æ–‡ä»¶å¤„ç†æœåŠ¡ï¼Œè¿”å› pd.DataFrame
    """
    result_df = df.copy()
    
    # H10ç«å“åˆ†ææœåŠ¡ ID
    H10_SERVICE_ID = "a8f3c2d1-4e5b-6c7d-8e9f-0a1b2c3d4e5f"
    
    # æ ¹æ®ä¸åŒçš„ service_id æ‰§è¡Œä¸åŒçš„å¤„ç†
    if service_id == H10_SERVICE_ID:
        # âœ… H10ç«å“åˆ†æå¤„ç†é€»è¾‘ï¼ˆç­‰å¾…ç”¨æˆ·æä¾›å…·ä½“é€»è¾‘ï¼‰
        print("ğŸ” H10ç«å“åˆ†æï¼šå¼€å§‹å¤„ç†æ•°æ®...")
        print(f"   æ•°æ®è¡Œæ•°: {len(df)}")
        print(f"   åˆ—å: {list(df.columns)}")
        # TODO: ç­‰å¾…ç”¨æˆ·æä¾›å…·ä½“çš„ç«å“åˆ†æé€»è¾‘
        # æš‚æ—¶è¿”å›åŸæ•°æ®ï¼Œæ·»åŠ ä¸€ä¸ªæç¤ºåˆ—
        result_df = df.copy()
        result_df["å¤„ç†çŠ¶æ€"] = "å¾…å®ç°ï¼šç­‰å¾…ç”¨æˆ·æä¾›åˆ†æé€»è¾‘"
        print("âš ï¸ è­¦å‘Š: H10ç«å“åˆ†æé€»è¾‘å°šæœªå®ç°ï¼Œè¿”å›åŸå§‹æ•°æ®")
        
    elif service_id == "h10" or service_id == "abfaf85c-9553-4d7b-9416-e3aff65e8587":  # Exå¤§å)
        # âœ… Exå¤§å å¤„ç†é€»è¾‘ï¼šè®¡ç®— 50ä¸ªè¯„è®ºä»¥å†…çš„ASINå æ¯”
        result_df = calculate_asin_ratio(df)
        
    elif service_id == "d144da99-d3e6-4b78-9cd5-70b1e4ced346":  # ç­›é€‰æ ¸å¿ƒå…³é”®è¯
        # âœ… ç­›é€‰æ ¸å¿ƒå…³é”®è¯é€»è¾‘
        result_df = filter_core_keywords(result_df)
        
    elif service_id == "65bb6f50-5087-488e-8f1b-350d4ed9fe00":  # è®¡ç®—æŠ•äº§æ¯”
        # âœ… è®¡ç®—æŠ•äº§æ¯”é€»è¾‘ï¼ˆè¿”å›æ–‡æœ¬æŠ¥å‘Šï¼‰
        return calculate_roi(df)
    
    # æ³¨æ„ï¼šç¤¾åª’é€‰å“æ³•æœåŠ¡ï¼ˆ7b83cf63-0ad0-4c11-8dc5-6d8c242fbfe6ï¼‰å·²åœ¨ /process ç«¯ç‚¹å¼€å§‹å¤„å¤„ç†ï¼Œä¸ä¼šåˆ°è¾¾è¿™é‡Œ
        
    else:
        # é»˜è®¤å¤„ç†
        result_df["å¤„ç†çŠ¶æ€"] = "å·²å¤„ç†"
    
    return result_df


def find_column(df: pd.DataFrame, possible_names: list, column_index: Optional[int] = None) -> Optional[str]:
    """
    æŸ¥æ‰¾åˆ—åï¼Œæ”¯æŒå¤šç§å¯èƒ½çš„åç§°æˆ–åˆ—ç´¢å¼•
    
    å‚æ•°:
    - df: DataFrame
    - possible_names: å¯èƒ½çš„åˆ—ååˆ—è¡¨ï¼ˆå¦‚ ["å…³é”®è¯", "A"]ï¼‰
    - column_index: åˆ—ç´¢å¼•ï¼ˆå¦‚ 0 è¡¨ç¤ºç¬¬1åˆ—ï¼Œ6 è¡¨ç¤ºç¬¬7åˆ—ï¼‰
    
    è¿”å›:
    - æ‰¾åˆ°çš„åˆ—åï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› None
    """
    # å…ˆå°è¯•æŒ‰åˆ—åæŸ¥æ‰¾
    for name in possible_names:
        if name in df.columns:
            return name
    
    # å¦‚æœæŒ‡å®šäº†åˆ—ç´¢å¼•ï¼Œå°è¯•ä½¿ç”¨ç´¢å¼•
    if column_index is not None and column_index < len(df.columns):
        return df.columns[column_index]
    
    return None


def parse_numeric(value) -> Optional[float]:
    """
    å°è¯•å°†å€¼è§£æä¸ºæ•°å€¼
    
    è¿”å›:
    - å¦‚æœæˆåŠŸè§£æä¸ºæ•°å€¼ï¼Œè¿”å› float
    - å¦‚æœæ— æ³•è§£æï¼Œè¿”å› None
    """
    if pd.isna(value):
        return None
    
    try:
        # å°è¯•ç›´æ¥è½¬æ¢ä¸ºæ•°å€¼
        return float(value)
    except (ValueError, TypeError):
        # å°è¯•æ¸…ç†å­—ç¬¦ä¸²åè½¬æ¢ï¼ˆç§»é™¤ç©ºæ ¼ã€é€—å·ç­‰ï¼‰
        try:
            cleaned = str(value).strip().replace(',', '').replace('ï¼Œ', '')
            return float(cleaned)
        except (ValueError, TypeError):
            return None


def filter_core_keywords(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç­›é€‰æ ¸å¿ƒå…³é”®è¯
    
    é€»è¾‘ï¼š
    1. æ‰¾åˆ°å…³é”®è¯åˆ—ã€ç›¸å…³äº§å“åˆ—ã€ABAå‘¨æ’ååˆ—
    2. æŒ‰"ç›¸å…³äº§å“"åˆ—é™åºæ’åºï¼ˆæ•°å€¼ä¼˜å…ˆï¼Œæ— æ³•è§£æçš„è§†ä¸ºæœ€å°å€¼ï¼‰
    3. åˆ é™¤"ABAå‘¨æ’å"ä¸ºç©ºæˆ–æ— æ•ˆçš„è¡Œ
    4. å–å‰60è¡Œ
    5. åªä¿ç•™å…³é”®è¯åˆ—
    """
    # 1. æ‰¾åˆ°æ‰€éœ€çš„åˆ—
    keyword_col = find_column(df, ["å…³é”®è¯", "A"], 0)  # ç¬¬1åˆ—ï¼ˆç´¢å¼•0ï¼‰
    related_product_col = find_column(df, ["ç›¸å…³äº§å“", "G"], 6)  # ç¬¬7åˆ—ï¼ˆç´¢å¼•6ï¼‰
    aba_rank_col = find_column(df, ["ABAå‘¨æ’å", "ABAå‘¨æ’å", "I"], 8)  # ç¬¬9åˆ—ï¼ˆç´¢å¼•8ï¼‰
    
    if not keyword_col:
        raise ValueError("æœªæ‰¾åˆ°å…³é”®è¯åˆ—ï¼ˆå…³é”®è¯ æˆ– Aåˆ—ï¼‰")
    if not related_product_col:
        raise ValueError("æœªæ‰¾åˆ°ç›¸å…³äº§å“åˆ—ï¼ˆç›¸å…³äº§å“ æˆ– Gåˆ—ï¼‰")
    if not aba_rank_col:
        raise ValueError("æœªæ‰¾åˆ°ABAå‘¨æ’ååˆ—ï¼ˆABAå‘¨æ’å æˆ– Iåˆ—ï¼‰")
    
    # 2. æŒ‰"ç›¸å…³äº§å“"åˆ—è¿›è¡Œé™åºæ’åº
    # ç­–ç•¥ï¼šåˆ†ç¦»æ•°å€¼å’Œéæ•°å€¼ï¼Œåˆ†åˆ«æ’åºååˆå¹¶
    # åˆ›å»ºè¾…åŠ©åˆ—ï¼šæ ‡è¯†æ˜¯å¦ä¸ºæ•°å€¼
    df['_is_numeric'] = df[related_product_col].apply(lambda x: parse_numeric(x) is not None)
    df['_numeric_val'] = df[related_product_col].apply(lambda x: parse_numeric(x) if parse_numeric(x) is not None else float('-inf'))
    df['_str_val'] = df[related_product_col].astype(str).str.strip()
    
    # åˆ†ç¦»æ•°å€¼å’Œéæ•°å€¼è¡Œ
    numeric_rows = df[df['_is_numeric']].copy()
    non_numeric_rows = df[~df['_is_numeric']].copy()
    
    # å¯¹æ•°å€¼è¡ŒæŒ‰æ•°å€¼é™åºæ’åº
    if len(numeric_rows) > 0:
        numeric_rows = numeric_rows.sort_values('_numeric_val', ascending=False)
    
    # å¯¹éæ•°å€¼è¡ŒæŒ‰å­—ç¬¦ä¸²é™åºæ’åº
    if len(non_numeric_rows) > 0:
        # ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒå®ç°é™åºï¼ˆç®€å•æ–¹æ³•ï¼šåè½¬å­—ç¬¦ä¸²åå‡åºæ’åºï¼Œå†åè½¬å›æ¥ï¼‰
        # æˆ–è€…ç›´æ¥ä½¿ç”¨è´Ÿçš„ Unicode ç ç‚¹å€¼
        non_numeric_rows['_str_sort_key'] = non_numeric_rows['_str_val'].apply(
            lambda x: tuple(-ord(c) for c in x[:10]) if x else (float('inf'),)
        )
        non_numeric_rows = non_numeric_rows.sort_values('_str_sort_key', ascending=True).drop('_str_sort_key', axis=1)
    
    # åˆå¹¶ï¼šæ•°å€¼è¡Œåœ¨å‰ï¼Œéæ•°å€¼è¡Œåœ¨å
    df = pd.concat([numeric_rows, non_numeric_rows], ignore_index=True)
    
    # æ¸…ç†è¾…åŠ©åˆ—
    df = df.drop(['_is_numeric', '_numeric_val', '_str_val'], axis=1)
    
    # 3. åˆ é™¤"ABAå‘¨æ’å"ä¸ºç©ºæˆ–æ— æ•ˆçš„è¡Œ
    invalid_values = ['-', 'â€”', 'NA', 'N/A', 'null', 'NULL', 'Null', '']
    
    def is_valid_aba_rank(value) -> bool:
        """åˆ¤æ–­ABAå‘¨æ’åæ˜¯å¦æœ‰æ•ˆ"""
        if pd.isna(value):
            return False
        
        value_str = str(value).strip()
        if not value_str:
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ•ˆå€¼ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
        if value_str.upper() in [v.upper() for v in invalid_values]:
            return False
        
        return True
    
    # è¿‡æ»¤æ— æ•ˆè¡Œ
    mask = df[aba_rank_col].apply(is_valid_aba_rank)
    df = df[mask].copy()
    
    # 4. å–å‰60è¡Œ
    df = df.head(60)
    
    # 5. åªä¿ç•™å…³é”®è¯åˆ—
    result_df = df[[keyword_col]].copy()
    
    # é‡å‘½ååˆ—ä¸º"å…³é”®è¯"ï¼ˆç»Ÿä¸€è¾“å‡ºæ ¼å¼ï¼‰
    result_df.columns = ['å…³é”®è¯']
    
    return result_df


def clean_numeric_value(value) -> float:
    """
    æ¸…æ´—æ•°å€¼ï¼šå»é™¤é€—å·ï¼Œè½¬æ¢ä¸ºæ•°å€¼ï¼Œæ— æ³•è§£æçš„æŒ‰ 0 å¤„ç†
    """
    if pd.isna(value):
        return 0.0
    
    try:
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå»é™¤é€—å·å’Œå…¶ä»–åˆ†éš”ç¬¦
        value_str = str(value).strip().replace(',', '').replace('ï¼Œ', '').replace(' ', '')
        # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        return float(value_str)
    except (ValueError, TypeError):
        return 0.0


def calculate_roi(df: pd.DataFrame):
    """
    è®¡ç®—æŠ•äº§æ¯”ï¼ˆROIï¼‰
    
    é€»è¾‘ï¼š
    1. è¯»å– Excel æ•°æ®å¹¶é€è¡Œéå†
    2. å¯¹å­—æ®µè¿›è¡Œæ•°å€¼æ¸…æ´—ï¼ˆå»é™¤é€—å·ï¼Œè½¬æ¢ä¸ºæ•°å€¼ï¼Œæ— æ³•è§£æçš„æŒ‰ 0 å¤„ç†ï¼‰
    3. ç´¯åŠ è®¡ç®—å…¨è¡¨æ•°æ®ï¼ˆæ€»ç‚¹å‡»é‡ã€æ€»è´­ä¹°é‡ï¼‰
    4. è®¡ç®—åŠ æƒç«ä»·åˆ†å­
    5. è¯»å–ç¬¬ä¸€ä¸ªå¤§äº 0 çš„äº§å“å‡ä»·ä½œä¸ºå‚è€ƒå®¢å•ä»·
    6. é˜²æ­¢é™¤ä»¥ 0
    7. è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡ï¼ˆå¹³å‡è½¬åŒ–ç‡ã€åŠ æƒå¹³å‡ç«ä»·ã€é¢„ä¼° ACOSï¼‰
    8. æ•´ç†ä¸ºæ–‡æœ¬åˆ†ææŠ¥å‘Š
    """
    # 1. æ‰¾åˆ°æ‰€éœ€çš„åˆ—ï¼ˆæ ¹æ® n8n é€»è¾‘ï¼‰
    click_col = find_column(df, ["å‘¨ç‚¹å‡»é‡", "E"], 4)  # Eåˆ—ï¼ˆç´¢å¼•4ï¼Œç¬¬5åˆ—ï¼‰
    purchase_col = find_column(df, ["å‘¨è´­ä¹°é‡", "F"], 5)  # Fåˆ—ï¼ˆç´¢å¼•5ï¼Œç¬¬6åˆ—ï¼‰
    bid_col = find_column(df, ["PPCç«ä»·-æœ€é«˜($)", "PPCç«ä»·-ä¸­ä½($)", "ç«ä»·-æ¨è", "ç«ä»·", "K"], 10)  # Kåˆ—ï¼ˆç´¢å¼•10ï¼Œç¬¬11åˆ—ï¼‰
    price_col = find_column(df, ["äº§å“å‡ä»·-å¹³å‡($)", "å‡ä»·-å¹³å‡", "äº§å“å‡ä»·", "å®¢å•ä»·", "R"], 17)  # Råˆ—ï¼ˆç´¢å¼•17ï¼Œç¬¬18åˆ—ï¼‰
    
    if not click_col:
        raise ValueError("æœªæ‰¾åˆ°å‘¨ç‚¹å‡»é‡åˆ—")
    if not purchase_col:
        raise ValueError("æœªæ‰¾åˆ°å‘¨è´­ä¹°é‡åˆ—")
    if not bid_col:
        raise ValueError("æœªæ‰¾åˆ°ç«ä»·åˆ—")
    if not price_col:
        raise ValueError("æœªæ‰¾åˆ°äº§å“å‡ä»·åˆ—")
    
    # 2. é€è¡Œéå†å¹¶æ¸…æ´—æ•°æ®
    total_clicks = 0.0
    total_purchases = 0.0
    weighted_bid_sum = 0.0  # åŠ æƒç«ä»·åˆ†å­ï¼šç«ä»· Ã— ç‚¹å‡»é‡ çš„ç´¯åŠ 
    reference_price = 0.0  # ç¬¬ä¸€ä¸ªå¤§äº 0 çš„äº§å“å‡ä»·
    
    for idx, row in df.iterrows():
        # æ¸…æ´—æ•°å€¼
        clicks = clean_numeric_value(row[click_col])
        purchases = clean_numeric_value(row[purchase_col])
        bid = clean_numeric_value(row[bid_col])
        price = clean_numeric_value(row[price_col])
        
        # ç´¯åŠ æ€»ç‚¹å‡»é‡å’Œæ€»è´­ä¹°é‡
        total_clicks += clicks
        total_purchases += purchases
        
        # è®¡ç®—åŠ æƒç«ä»·åˆ†å­ï¼ˆå½“ç‚¹å‡»é‡ > 0 ä¸”ç«ä»· > 0 æ—¶ï¼‰
        if clicks > 0 and bid > 0:
            weighted_bid_sum += bid * clicks
        
        # è¯»å–ç¬¬ä¸€ä¸ªå¤§äº 0 çš„äº§å“å‡ä»·ä½œä¸ºå‚è€ƒå®¢å•ä»·
        if reference_price == 0.0 and price > 0:
            reference_price = price
    
    # 3. é˜²æ­¢é™¤ä»¥ 0
    if total_clicks == 0:
        total_clicks = 1.0
    if reference_price == 0.0:
        reference_price = 1.0
    
    # 4. è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
    # å¹³å‡è½¬åŒ–ç‡ (%) = (æ€»è´­ä¹°é‡ Ã· æ€»ç‚¹å‡»é‡) Ã— 100
    conversion_rate = (total_purchases / total_clicks) * 100
    
    # åŠ æƒå¹³å‡ç«ä»· = (ç«ä»· Ã— ç‚¹å‡»é‡ä¹‹å’Œ) Ã· æ€»ç‚¹å‡»é‡
    weighted_avg_bid = weighted_bid_sum / total_clicks
    
    # é¢„ä¼° ACOS (%) = åŠ æƒå¹³å‡ç«ä»· Ã· (å®¢å•ä»· Ã— è½¬åŒ–ç‡) Ã— 100
    # æ³¨æ„ï¼šè½¬åŒ–ç‡éœ€è¦è½¬æ¢ä¸ºå°æ•°ï¼ˆé™¤ä»¥ 100ï¼‰
    if conversion_rate > 0:
        estimated_acos = (weighted_avg_bid / (reference_price * (conversion_rate / 100))) * 100
    else:
        estimated_acos = 0.0
    
    # 5. æ•´ç†ä¸ºæ–‡æœ¬åˆ†ææŠ¥å‘Šï¼ˆæ ¹æ® n8n é€»è¾‘æ ¼å¼ï¼‰
    report = f"""ğŸ“Š å–å®¶ç²¾çµå…³é”®è¯åˆ†ææŠ¥å‘Š

ğŸ”¹ æ•°æ®è¡Œæ•°: {len(df)} è¡Œ
ğŸ”¹ æ€»ç‚¹å‡»é‡: {total_clicks:,.0f} æ¬¡
ğŸ”¹ æ€»è´­ä¹°é‡: {total_purchases:,.0f} å•
ğŸ”¹ å‚è€ƒå®¢å•ä»·: ${reference_price:.2f}

ğŸ“ˆ å¹³å‡ç‚¹å‡»è½¬åŒ–ç‡: {conversion_rate:.2f}%
ğŸ’° åŠ æƒå¹³å‡å»ºè®®ç«ä»·: ${weighted_avg_bid:.2f}"""
    
    if conversion_rate == 0:
        report += f"\nğŸ“‰ é¢„ä¼°æ€» ACOS: æ— æ³•è®¡ç®— (æ— è½¬åŒ–)"
    else:
        report += f"\nğŸ“‰ é¢„ä¼°æ€» ACOS: {estimated_acos:.2f}%"
    
    return report


def get_product_reviews_count(asin: str, max_retries: int = 2) -> int:
    """
    ä½¿ç”¨ RapidAPI è·å–äº§å“çš„è¯„è®ºæ•°
    
    å‚æ•°:
    - asin: äº§å“ ASIN
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
    - int: è¯„è®ºæ•°ï¼Œå¦‚æœå¤±è´¥è¿”å› 0
    """
    url = f"https://{RAPIDAPI_HOST}/top-product-reviews"
    params = {
        'asin': asin,
        'country': 'US'
    }
    headers = {
        'x-rapidapi-host': RAPIDAPI_HOST,
        'x-rapidapi-key': RAPIDAPI_KEY
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            # ä» API å“åº”ä¸­æå–è¯„è®ºæ•°
            # æ ¹æ® RapidAPI çš„å“åº”æ ¼å¼ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¿™é‡Œçš„è§£æé€»è¾‘
            # é€šå¸¸è¯„è®ºæ•°å¯èƒ½åœ¨ data['reviews'] çš„é•¿åº¦ï¼Œæˆ–è€… data['total_reviews'] ç­‰å­—æ®µ
            if 'data' in data and isinstance(data['data'], list):
                # å¦‚æœè¿”å›çš„æ˜¯è¯„è®ºåˆ—è¡¨ï¼Œä½¿ç”¨åˆ—è¡¨é•¿åº¦
                review_count = len(data['data'])
            elif 'total_reviews' in data:
                review_count = int(data['total_reviews'])
            elif 'reviews' in data and isinstance(data['reviews'], list):
                review_count = len(data['reviews'])
            elif 'rating_count' in data:
                review_count = int(data['rating_count'])
            else:
                # å°è¯•ä»å“åº”ä¸­æŸ¥æ‰¾è¯„è®ºæ•°å­—æ®µ
                review_count = 0
                # å¯ä»¥æ·»åŠ æ›´å¤šè§£æé€»è¾‘
            
            return review_count
            
        except requests.exceptions.RequestException as e:
            print(f"  âš ï¸ è·å– ASIN {asin} è¯„è®ºæ•°å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(0.5)  # çŸ­æš‚å»¶è¿Ÿåé‡è¯•
        except Exception as e:
            print(f"  âš ï¸ è§£æ ASIN {asin} è¯„è®ºæ•°å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(0.5)
    
    return 0


def search_amazon_natural_products(keyword: str, max_retries: int = 3) -> List[Dict[str, any]]:
    """
    åœ¨ Amazon ç¾å›½ç«™æœç´¢å…³é”®è¯ï¼Œè·å–é¦–é¡µè‡ªç„¶ä½ ASIN åŠå…¶è¯„è®ºæ•°
    ä¼˜å…ˆå°è¯• RapidAPIï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨çˆ¬è™«æ–¹å¼ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
    
    å‚æ•°:
    - keyword: æœç´¢å…³é”®è¯
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
    - List[Dict]: æ¯ä¸ªå…ƒç´ åŒ…å« {'asin': 'B0XXX', 'ratingCount': 123} æˆ–ç©ºåˆ—è¡¨ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    # ç›´æ¥ä½¿ç”¨çˆ¬è™«æ–¹å¼ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
    # æ³¨æ„ï¼šRapidAPI å¯èƒ½æ²¡æœ‰æœç´¢ç«¯ç‚¹ï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨çˆ¬è™«
    url = f"https://www.amazon.com/s?k={quote(keyword)}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.9',
        'Referer': 'https://www.amazon.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    for attempt in range(max_retries):
        try:
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬è™«
            if attempt > 0:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿ï¼š2ç§’ã€4ç§’ã€8ç§’
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # è·å– HTML æ–‡æœ¬ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
            html = response.text
            
            # Step 1ï¸âƒ£ï¼šåˆ‡åˆ†æœç´¢ç»“æœå—ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ‡åˆ†åŒ…å« data-component-type="s-search-result" çš„ div
            blocks = re.split(r'<div[^>]+data-component-type="s-search-result"[^>]*>', html, flags=re.IGNORECASE)
            blocks = blocks[1:]  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºå—ï¼ˆåˆ‡åˆ†å‰çš„éƒ¨åˆ†ï¼‰
            
            items = []
            
            # Step 2ï¸âƒ£ï¼šå¾ªç¯è§£æ ASIN + è¯„è®ºæ•°ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
            for raw in blocks:
                # æå– ASINï¼ˆ10ä½å­—æ¯æ•°å­—ï¼‰
                asin_match = re.search(r'data-asin="([A-Z0-9]{10})"', raw, re.IGNORECASE)
                if not asin_match:
                    continue
                
                asin = asin_match.group(1)
                
                # è·³è¿‡å¹¿å‘Šï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼šæ£€æµ‹å¤šä¸ªå¹¿å‘Šæ ‡è¯†ï¼‰
                is_sponsored = bool(re.search(
                    r'sp-sponsored-result|AdHolder|SponsoredAd|aria-label="Sponsored"|>Sponsored<',
                    raw,
                    re.IGNORECASE
                ))
                if is_sponsored:
                    continue
                
                # æå–è¯„è®ºæ•°ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼šå°è¯•å¤šç§æ¨¡å¼ï¼‰
                rating_count = 0
                
                # æ–¹æ³•1ï¼šaria-label="X ratings"ï¼ˆæœ€å‡†ç¡®ï¼‰
                rating_match = re.search(r'aria-label="([\d,]+)\s+ratings?"', raw, re.IGNORECASE)
                if rating_match:
                    try:
                        rating_count = int(rating_match.group(1).replace(',', ''))
                    except (ValueError, AttributeError):
                        pass
                
                # æ–¹æ³•2ï¼š>X ratings</a>
                if rating_count == 0:
                    rating_match = re.search(r'>\s*([\d,]+)\s+ratings?\s*</a>', raw, re.IGNORECASE)
                    if rating_match:
                        try:
                            rating_count = int(rating_match.group(1).replace(',', ''))
                        except (ValueError, AttributeError):
                            pass
                
                # æ–¹æ³•3ï¼šæŸ¥æ‰¾ "X ratings" æˆ– "X reviews"ï¼ˆæ›´å®½æ¾ï¼‰
                if rating_count == 0:
                    alt_match = re.search(r'([\d,]+)\s*(?:ratings?|reviews?)', raw, re.IGNORECASE)
                    if alt_match:
                        try:
                            rating_count = int(alt_match.group(1).replace(',', ''))
                        except (ValueError, AttributeError):
                            pass
                
                # æ–¹æ³•4ï¼šæŸ¥æ‰¾ data-rating-count å±æ€§
                if rating_count == 0:
                    attr_match = re.search(r'data-rating-count="([\d,]+)"', raw, re.IGNORECASE)
                    if attr_match:
                        try:
                            rating_count = int(attr_match.group(1).replace(',', ''))
                        except (ValueError, AttributeError):
                            pass
                
                # æ–¹æ³•5ï¼šæŸ¥æ‰¾ span ä¸­çš„æ•°å­—æ¨¡å¼ï¼ˆæ›´å®½æ¾ï¼‰
                if rating_count == 0:
                    # æŸ¥æ‰¾ç±»ä¼¼ "1,234" è¿™æ ·çš„æ•°å­—ï¼Œåé¢è·Ÿç€ ratings/reviews
                    number_match = re.search(r'([\d]{1,3}(?:,\d{3})*)\s*(?:ratings?|reviews?)', raw, re.IGNORECASE)
                    if number_match:
                        try:
                            rating_count = int(number_match.group(1).replace(',', ''))
                        except (ValueError, AttributeError):
                            pass
                
                # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾ä»»ä½•åŒ…å«æ•°å­—å’Œ ratings çš„æ–‡æœ¬
                if rating_count == 0:
                    # æ›´å®½æ¾çš„åŒ¹é…ï¼šä»»ä½•æ•°å­—åè·Ÿ ratings/reviews
                    loose_match = re.search(r'(\d+(?:,\d+)*)\s*(?:ratings?|reviews?)', raw, re.IGNORECASE)
                    if loose_match:
                        try:
                            rating_count = int(loose_match.group(1).replace(',', ''))
                        except (ValueError, AttributeError):
                            pass
                
                items.append({
                    'asin': asin,
                    'ratingCount': rating_count
                })
                
                # è°ƒè¯•ï¼šæ‰“å°å‰å‡ ä¸ªäº§å“çš„ä¿¡æ¯
                if len(items) <= 3:
                    print(f"    ğŸ“Œ ASIN: {asin}, è¯„è®ºæ•°: {rating_count}")
            
            # å¦‚æœæ‰¾åˆ°äº†äº§å“ï¼Œè¿”å›ç»“æœ
            if items:
                print(f"  âœ… æ‰¾åˆ° {len(items)} ä¸ªè‡ªç„¶ä½äº§å“")
                print(f"  ğŸ“Š è¯„è®ºæ•°ç»Ÿè®¡: æœ€å°={min(p['ratingCount'] for p in items)}, æœ€å¤§={max(p['ratingCount'] for p in items)}, å¹³å‡={sum(p['ratingCount'] for p in items) / len(items):.1f}")
                return items
            else:
                print(f"  âš ï¸ æœªæ‰¾åˆ°ä»»ä½•äº§å“")
                # è°ƒè¯•ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ‰€æœ‰ç»“æœéƒ½æ˜¯å¹¿å‘Š
                sponsored_count = 0
                for raw in blocks[:10]:  # åªæ£€æŸ¥å‰10ä¸ªå—
                    if re.search(r'sp-sponsored-result|AdHolder|SponsoredAd|aria-label="Sponsored"|>Sponsored<', raw, re.IGNORECASE):
                        sponsored_count += 1
                print(f"  ğŸ” è°ƒè¯•ä¿¡æ¯: å‰10ä¸ªå—ä¸­æœ‰ {sponsored_count} ä¸ªå¹¿å‘Šä½")
                return []
            
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Amazon æœç´¢è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt == max_retries - 1:
                print(f"âŒ æ— æ³•è·å–å…³é”®è¯ '{keyword}' çš„æœç´¢ç»“æœ")
                return []
        except Exception as e:
            print(f"âš ï¸ è§£æ Amazon æœç´¢ç»“æœå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt == max_retries - 1:
                print(f"âŒ æ— æ³•è§£æå…³é”®è¯ '{keyword}' çš„æœç´¢ç»“æœ")
                import traceback
                traceback.print_exc()
                return []
    
    return []


def search_amazon_products_by_keyword_rapidapi(keyword: str, max_retries: int = 2) -> List[str]:
    """
    ä½¿ç”¨ RapidAPI æœç´¢å…³é”®è¯ï¼Œè·å–é¦–é¡µè‡ªç„¶ä½ ASIN åˆ—è¡¨
    
    å‚æ•°:
    - keyword: æœç´¢å…³é”®è¯
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
    - List[str]: ASIN åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥è¿”å›ç©ºåˆ—è¡¨
    """
    # RapidAPI çš„æœç´¢ç«¯ç‚¹
    # æ³¨æ„ï¼šå¦‚æœç«¯ç‚¹ä¸å¯¹ï¼Œè¯·ä¿®æ”¹è¿™é‡Œçš„ URL
    # å¸¸è§çš„ç«¯ç‚¹å¯èƒ½æ˜¯ï¼š/search-products, /product-search, /search ç­‰
    url = f"https://{RAPIDAPI_HOST}/search-products"
    params = {
        'query': keyword,
        'country': 'US',
        'page': 1,
        'page_size': 20  # è·å–é¦–é¡µç»“æœï¼ˆé€šå¸¸ 16-20 ä¸ªï¼‰
    }
    headers = {
        'x-rapidapi-host': RAPIDAPI_HOST,
        'x-rapidapi-key': RAPIDAPI_KEY
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, params=params, headers=headers, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            # ä» API å“åº”ä¸­æå– ASIN åˆ—è¡¨
            # æ ¹æ® RapidAPI çš„å“åº”æ ¼å¼ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¿™é‡Œçš„è§£æé€»è¾‘
            asins = []
            
            if 'data' in data and isinstance(data['data'], list):
                for item in data['data']:
                    if 'asin' in item:
                        asins.append(item['asin'])
                    elif 'product_id' in item:
                        asins.append(item['product_id'])
            elif 'products' in data and isinstance(data['products'], list):
                for item in data['products']:
                    if 'asin' in item:
                        asins.append(item['asin'])
                    elif 'product_id' in item:
                        asins.append(item['product_id'])
            elif 'results' in data and isinstance(data['results'], list):
                for item in data['results']:
                    if 'asin' in item:
                        asins.append(item['asin'])
                    elif 'product_id' in item:
                        asins.append(item['product_id'])
            
            # è¿‡æ»¤æ‰å¹¿å‘Šä½ï¼ˆå¦‚æœ API è¿”å›äº† is_sponsored å­—æ®µï¼‰
            # æ³¨æ„ï¼šRapidAPI å¯èƒ½ä¸åŒºåˆ†å¹¿å‘Šå’Œè‡ªç„¶ä½ï¼Œè¿™é‡Œä¿ç•™æ‰€æœ‰ç»“æœ
            # å¦‚æœ API æä¾›äº† is_sponsored å­—æ®µï¼Œå¯ä»¥åœ¨è¿™é‡Œè¿‡æ»¤
            
            if asins:
                print(f"  âœ… æ‰¾åˆ° {len(asins)} ä¸ªäº§å“ ASIN")
                return asins
            else:
                print(f"  âš ï¸ æœªæ‰¾åˆ°ä»»ä½•äº§å“ ASIN")
                return []
            
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ RapidAPI æœç´¢è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(1)
        except Exception as e:
            print(f"âš ï¸ è§£æ RapidAPI æœç´¢ç»“æœå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(1)
            import traceback
            traceback.print_exc()
    
    return []


# æ³¨æ„ï¼šä»¥ä¸‹å‡½æ•°å·²åºŸå¼ƒï¼Œä¿ç•™çº¯çˆ¬è™«ç‰ˆæœ¬ï¼ˆsearch_amazon_natural_productsï¼‰
# å› ä¸º RapidAPI å¯èƒ½æ²¡æœ‰æœç´¢ç«¯ç‚¹ï¼Œä¸”æ— æ³•åŒºåˆ†å¹¿å‘Šä½
    """
    ä½¿ç”¨çˆ¬è™«æœç´¢å…³é”®è¯ï¼Œè·å–é¦–é¡µè‡ªç„¶ä½ ASIN åˆ—è¡¨ï¼ˆæ’é™¤å¹¿å‘Šï¼‰
    å‚è€ƒ n8n é€»è¾‘ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ‡åˆ† HTML å—å¹¶è§£æ
    
    å‚æ•°:
    - keyword: æœç´¢å…³é”®è¯
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
    - List[str]: ASIN åˆ—è¡¨ï¼Œå¦‚æœå¤±è´¥è¿”å›ç©ºåˆ—è¡¨
    """
    url = f"https://www.amazon.com/s?k={quote(keyword)}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.9',
        'Referer': 'https://www.amazon.com/',
    }
    
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                time.sleep(1)  # é‡è¯•æ—¶å»¶è¿Ÿ
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # è·å– HTML æ–‡æœ¬ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
            html = response.text
            
            # Step 1ï¸âƒ£ï¼šåˆ‡åˆ†æœç´¢ç»“æœå—ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
            blocks = re.split(r'<div[^>]+data-component-type="s-search-result"[^>]*>', html, flags=re.IGNORECASE)
            blocks = blocks[1:]  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºå—
            
            asins = []
            
            # Step 2ï¸âƒ£ï¼šå¾ªç¯è§£æ ASINï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
            for raw in blocks:
                # æå– ASINï¼ˆ10ä½å­—æ¯æ•°å­—ï¼‰
                asin_match = re.search(r'data-asin="([A-Z0-9]{10})"', raw, re.IGNORECASE)
                if not asin_match:
                    continue
                
                asin = asin_match.group(1)
                
                # è·³è¿‡å¹¿å‘Šï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼šæ£€æµ‹å¤šä¸ªå¹¿å‘Šæ ‡è¯†ï¼‰
                is_sponsored = bool(re.search(
                    r'sp-sponsored-result|AdHolder|SponsoredAd|aria-label="Sponsored"|>Sponsored<',
                    raw,
                    re.IGNORECASE
                ))
                if is_sponsored:
                    continue
                
                asins.append(asin)
            
            if asins:
                print(f"  âœ… æ‰¾åˆ° {len(asins)} ä¸ªè‡ªç„¶ä½ ASIN")
                return asins
            else:
                print(f"  âš ï¸ æœªæ‰¾åˆ°ä»»ä½•è‡ªç„¶ä½ ASIN")
                return []
            
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Amazon æœç´¢è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt == max_retries - 1:
                return []
        except Exception as e:
            print(f"âš ï¸ è§£æ Amazon æœç´¢ç»“æœå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt == max_retries - 1:
                import traceback
                traceback.print_exc()
                return []
    
    return []


def calculate_asin_ratio(df: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®— 50ä¸ªè¯„è®ºä»¥å†…çš„ASINå æ¯”
    
    é€»è¾‘ï¼š
    1. æ‰¾åˆ°"å…³é”®è¯"åˆ—ï¼ˆä½œä¸ºä¸»é”®ï¼‰
    2. å¯¹æ¯ä¸€è¡Œå…³é”®è¯ï¼Œåœ¨ Amazon æœç´¢å¹¶è·å–é¦–é¡µè‡ªç„¶ä½ ASIN
    3. ç»Ÿè®¡é¦–é¡µè‡ªç„¶ä½ ASIN æ€»æ•°
    4. ç­›é€‰è¯„è®ºæ•° < 50 çš„ ASIN
    5. è®¡ç®—å æ¯”
    6. åœ¨åŸè¡¨ä¸­æ’å…¥æ–°åˆ—"50ä¸ªè¯„è®ºä»¥å†…çš„ASINå æ¯”"
    """
    # 1. æ‰¾åˆ°"å…³é”®è¯"åˆ—
    keyword_col = find_column(df, ["å…³é”®è¯", "å…³é”®è¯åˆ—", "Keyword", "A"], 0)
    
    if not keyword_col:
        raise ValueError("æœªæ‰¾åˆ°'å…³é”®è¯'åˆ—ï¼Œè¯·ç¡®ä¿ Excel æ–‡ä»¶åŒ…å«'å…³é”®è¯'åˆ—")
    
    # 2. åˆ›å»ºç»“æœ DataFrameï¼ˆå¤åˆ¶åŸè¡¨ï¼‰
    result_df = df.copy()
    
    # 3. ç¡®å®šæ–°åˆ—çš„æ’å…¥ä½ç½®ï¼ˆå…³é”®è¯åˆ—çš„å³ä¾§ï¼Œå³ç´¢å¼• +1ï¼‰
    keyword_col_index = list(result_df.columns).index(keyword_col)
    new_col_name = "50ä¸ªè¯„è®ºä»¥å†…çš„ASINå æ¯”"
    
    # 4. åˆå§‹åŒ–æ–°åˆ—
    result_df[new_col_name] = "0.00%"
    
    # 5. é€è¡Œå¤„ç†å…³é”®è¯
    total_rows = len(result_df)
    print(f"ğŸ“Š å¼€å§‹å¤„ç† {total_rows} ä¸ªå…³é”®è¯...")
    
    for idx, row in result_df.iterrows():
        keyword = str(row[keyword_col]).strip()
        
        if not keyword or keyword == 'nan' or keyword == '':
            print(f"âš ï¸ ç¬¬ {idx + 1} è¡Œï¼šå…³é”®è¯ä¸ºç©ºï¼Œè·³è¿‡")
            continue
        
        print(f"ğŸ” [{idx + 1}/{total_rows}] å¤„ç†å…³é”®è¯: {keyword}")
        
        try:
            # æœç´¢ Amazon è·å–é¦–é¡µè‡ªç„¶ä½ ASIN
            products = search_amazon_natural_products(keyword)
            
            if not products:
                print(f"  âš ï¸ æœªæ‰¾åˆ°æœç´¢ç»“æœï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.00%")
                result_df.at[idx, new_col_name] = "0.00%"
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(1)
                continue
            
            # Step 1ï¸âƒ£ è¿‡æ»¤ Sponsored å¹¿å‘Šå—ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼šäºŒæ¬¡è¿‡æ»¤ï¼‰
            # æ³¨æ„ï¼šsearch_amazon_natural_products å·²ç»è¿‡æ»¤äº†å¤§éƒ¨åˆ†å¹¿å‘Šï¼Œè¿™é‡Œä¿ç•™æ‰€æœ‰
            filtered = products  # å·²ç»è¿‡æ»¤è¿‡äº†
            
            # è°ƒè¯•ï¼šæ‰“å°äº§å“ä¿¡æ¯
            print(f"  ğŸ“Š è·å–åˆ° {len(filtered)} ä¸ªè‡ªç„¶ä½äº§å“")
            if len(filtered) > 0:
                rating_counts = [p['ratingCount'] for p in filtered]
                print(f"  ğŸ“Š è¯„è®ºæ•°èŒƒå›´: æœ€å°={min(rating_counts)}, æœ€å¤§={max(rating_counts)}, å¹³å‡={sum(rating_counts)/len(rating_counts):.1f}")
                print(f"  ğŸ“Š è¯„è®ºæ•°è¯¦æƒ…ï¼ˆå‰5ä¸ªï¼‰: {[p['ratingCount'] for p in filtered[:5]]}")
            
            # Step 2ï¸âƒ£ æ‰¾å‡ºè¯„è®ºæ•°ä½äº 50 çš„è‡ªç„¶ä½ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
            low_ratings = [p for p in filtered if p['ratingCount'] < 50]
            
            print(f"  ğŸ“Š è¯„è®ºæ•° < 50 çš„äº§å“æ•°: {len(low_ratings)}")
            
            # Step 3ï¸âƒ£ è®¡ç®—å æ¯”ï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
            # åˆ†å­ï¼šä½è¯„è®ºæ•°ï¼ˆ<50ï¼‰çš„è‡ªç„¶ä½äº§å“æ•°é‡
            numerator = len(low_ratings)
            # åˆ†æ¯ï¼šå»é™¤å¹¿å‘Šåé¦–é¡µè‡ªç„¶ä½æ€»æ•°
            denominator = len(filtered)
            
            # è®¡ç®—å æ¯”ï¼ˆç™¾åˆ†æ¯”ï¼Œä¿ç•™ 2 ä½å°æ•°ï¼Œå‚è€ƒ n8n é€»è¾‘ï¼‰
            if denominator > 0:
                ratio = (numerator / denominator) * 100
                ratio_percent = f"{ratio:.2f}%"
            else:
                ratio_percent = "0.00%"
            
            result_df.at[idx, new_col_name] = ratio_percent
            print(f"  âœ… å®Œæˆï¼šæ€»ASIN={denominator}, ä½è¯„è®ºASIN={numerator}, å æ¯”={ratio_percent}")
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«ï¼ˆæ¯ä¸ªå…³é”®è¯ä¹‹é—´å»¶è¿Ÿ 2 ç§’ï¼‰
            time.sleep(2)
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {str(e)}")
            result_df.at[idx, new_col_name] = "0.00%"
            import traceback
            traceback.print_exc()
            # å³ä½¿å‡ºé”™ä¹Ÿæ·»åŠ å»¶è¿Ÿ
            time.sleep(1)
    
    # 6. å°†æ–°åˆ—æ’å…¥åˆ°å…³é”®è¯åˆ—çš„å³ä¾§
    cols = list(result_df.columns)
    cols.remove(new_col_name)
    insert_index = keyword_col_index + 1
    cols.insert(insert_index, new_col_name)
    result_df = result_df[cols]
    
    print(f"âœ… å¤„ç†å®Œæˆï¼Œå·²æ·»åŠ æ–°åˆ— '{new_col_name}'")
    return result_df


@app.post("/webhook/{webhook_path:path}")
async def webhook_handler(
    webhook_path: str,
    file: Optional[UploadFile] = File(None),
    data: Optional[str] = Form(None)
):
    """
    å…¼å®¹ n8n webhook æ ¼å¼çš„æ¥å£
    æ”¯æŒé€šè¿‡è·¯å¾„åŒºåˆ†ä¸åŒçš„æœåŠ¡
    """
    # æ ¹æ® webhook_path æ˜ å°„åˆ° service_id
    path_mapping = {
        "h10": "abfaf85c-9553-4d7b-9416-e3aff65e8587",
        "test-hook": "d144da99-d3e6-4b78-9cd5-70b1e4ced346",
        "d6898f17-a3dd-4171-9a74-24e5cbe67e16": "65bb6f50-5087-488e-8f1b-350d4ed9fe00",
    }
    
    service_id = path_mapping.get(webhook_path)
    input_text = None
    
    if data:
        try:
            data_dict = json.loads(data)
            input_text = data_dict.get("input_text")
        except:
            input_text = data
    
    if file:
        return await process_excel(file=file, service_id=service_id, input_text=input_text)
    else:
        raise HTTPException(status_code=400, detail="éœ€è¦ä¸Šä¼ æ–‡ä»¶")


@app.get("/")
def read_root():
    return {
        "status": "running",
        "message": "Python Backend is Running!",
        "endpoints": {
            "/process": "å¤„ç† Excel æ–‡ä»¶",
            "/webhook/{path}": "Webhook æ¥å£ï¼ˆå…¼å®¹ n8nï¼‰",
            "/docs": "API æ–‡æ¡£"
        }
    }


@app.get("/health")
def health_check():
    return {"status": "healthy", "supabase_connected": supabase is not None}


# ============================================
# ç¤¾åª’é€‰å“æ³•æœåŠ¡ API ç«¯ç‚¹
# ============================================

@app.post("/api/jobs")
async def create_research_job(
    keyword: str = Form(...),
    service_id: Optional[str] = Form(None)
):
    """
    åˆ›å»ºå¸‚åœºè°ƒç ”ä»»åŠ¡
    
    å‚æ•°:
    - keyword: è°ƒç ”å…³é”®è¯
    - service_id: æœåŠ¡IDï¼ˆå¯é€‰ï¼‰
    """
    job_id = str(uuid.uuid4())
    job_storage[job_id] = {
        "job_id": job_id,
        "status": "queued",
        "keyword": keyword.strip(),
        "progress": 0.0,
        "sections": [{"title": f"ç« èŠ‚{i+1}", "state": "pending"} for i in range(18)],
        "created_at": datetime.now().isoformat(),
        "artifacts": {
            "report_url": None,
            "image_url": None
        }
    }
    
    # å¯åŠ¨åå°ä»»åŠ¡ï¼ˆä½¿ç”¨ asyncio åœ¨åå°æ‰§è¡Œï¼‰
    asyncio.create_task(execute_research_job(job_id, keyword.strip()))
    
    return {"job_id": job_id, "status": "queued"}


@app.get("/api/jobs/{job_id}")
async def get_job_status(job_id: str):
    """
    æŸ¥è¯¢ä»»åŠ¡è¿›åº¦
    """
    if job_id not in job_storage:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    job = job_storage[job_id]
    return {
        "job_id": job_id,
        "status": job["status"],
        "progress": job["progress"],
        "sections": job["sections"],
        "artifacts": job["artifacts"]
    }


@app.get("/api/jobs/{job_id}/report")
async def download_report(job_id: str):
    """
    ä¸‹è½½ Word æŠ¥å‘Š
    """
    if job_id not in job_storage:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    job = job_storage[job_id]
    if "report_path" not in job.get("artifacts", {}):
        raise HTTPException(status_code=404, detail="æŠ¥å‘Šå°šæœªç”Ÿæˆ")
    
    report_path = job["artifacts"]["report_path"]
    if not os.path.exists(report_path):
        raise HTTPException(status_code=404, detail="æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨")
    
    with open(report_path, "rb") as f:
        content = f.read()
    
    return StreamingResponse(
        io.BytesIO(content),
        media_type="application/msword",
        headers={
            "Content-Disposition": 'attachment; filename="Market_Research_Report.doc"'
        }
    )


@app.get("/api/jobs/{job_id}/image")
async def get_image(job_id: str):
    """
    è·å–ç”Ÿæˆçš„å›¾ç‰‡
    """
    if job_id not in job_storage:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    job = job_storage[job_id]
    if "image_path" not in job.get("artifacts", {}):
        raise HTTPException(status_code=404, detail="å›¾ç‰‡å°šæœªç”Ÿæˆ")
    
    image_path = job["artifacts"]["image_path"]
    if not os.path.exists(image_path):
        raise HTTPException(status_code=404, detail="å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨")
    
    with open(image_path, "rb") as f:
        content = f.read()
    
    return StreamingResponse(
        io.BytesIO(content),
        media_type="image/png",
        headers={
            "Content-Disposition": f'inline; filename="product_image.png"'
        }
    )


# ============================================
# ç¤¾åª’é€‰å“æ³•æœåŠ¡æ ¸å¿ƒé€»è¾‘
# ============================================

# 18 ä¸ªç« èŠ‚ä»»åŠ¡å®šä¹‰ï¼ˆä¸ n8n å®Œå…¨ä¸€è‡´ï¼‰
RESEARCH_TASKS = [
    {"section_title": "è°ƒç ”æ€»ç»“", "search_query_template": "{keyword} market opportunity analysis growth drivers", "writing_instruction_template": "å¸‚åœºå®¹é‡è¶‹åŠ¿ã€æ˜¯å¦å€¼å¾—è¿›å…¥ï¼ˆæ˜ç¡®ç»“è®ºï¼‰ã€ç»†åˆ†ç­–ç•¥ã€å·®å¼‚åŒ–è·¯å¾„ã€å¼ºç›¸å…³æ‰§è¡Œå»ºè®®"},
    {"section_title": "å¸‚åœºå®¹é‡", "search_query_template": "{keyword} market size CAGR seasonal trends", "writing_instruction_template": "å¸‚åœºè§„æ¨¡ã€ç”Ÿå‘½å‘¨æœŸã€å­£èŠ‚æ€§ï¼›æ— ç›´æ¥æ•°æ®ç»™æ›¿ä»£æ–¹æ¡ˆï¼ˆç«å“æ¨ä¼°/å…³è”å¸‚åœºå¤–æ¨ï¼‰"},
    {"section_title": "å¸‚åœºç«äº‰", "search_query_template": "{keyword} top brands competitors market share", "writing_instruction_template": "Top5 å“ç‰Œï¼šå›½å®¶ã€æˆç«‹æ—¶é—´ã€å®šä½ã€æ ¸å¿ƒç«äº‰åŠ›ã€å°è¯•æ¨ç®—ä»½é¢"},
    {"section_title": "äº§å“è®¤çŸ¥", "search_query_template": "what is {keyword} definition types usage", "writing_instruction_template": "åŠŸèƒ½/æè´¨/å®‰å…¨/åœºæ™¯/è¶‹åŠ¿/ä»·æ ¼/ç—›ç‚¹ç­‰ç§‘æ™®"},
    {"section_title": "äº§å“åŠŸèƒ½", "search_query_template": "{keyword} features categories cost types", "writing_instruction_template": "ä¸»æµæ¬¾å¼åŠŸèƒ½å·®å¼‚ï¼›å¹³å‡æˆæœ¬ç»“æ„æ¨ä¼°"},
    {"section_title": "äº§å“ç»“æ„", "search_query_template": "{keyword} materials construction components", "writing_instruction_template": "BOM æ‹†è§£ï¼šææ–™ã€ç»“æ„å±‚æ¬¡ã€æ ¸å¿ƒéƒ¨ä»¶ã€æˆæœ¬å æ¯”ä¼°ç®—"},
    {"section_title": "è¶‹åŠ¿å…ƒç´ ", "search_query_template": "{keyword} design trends 2024 2025", "writing_instruction_template": "é¢œè‰²ã€å¤–è§‚ã€å·¥è‰ºã€å‚æ•°ã€æ™ºèƒ½åŒ–è¶‹åŠ¿"},
    {"section_title": "å—ä¼—ç‰¹å¾", "search_query_template": "{keyword} buyer persona demographics", "writing_instruction_template": "å¹´é¾„ã€æ€§åˆ«ã€èŒä¸šã€æ”¶å…¥ã€æ•™è‚²ã€åå¥½ã€åœºæ™¯"},
    {"section_title": "å—ä¼—éœ€æ±‚", "search_query_template": "{keyword} customer needs buying factors wishlist", "writing_instruction_template": "Top20 è´­ä¹°åŠ¨æœº + Top20 æœªæ»¡è¶³éœ€æ±‚ï¼Œåšå æ¯”æ’åº"},
    {"section_title": "å—ä¼—è´­ä¹°äº§å“", "search_query_template": "{keyword} frequently bought together accessories", "writing_instruction_template": "äº’è¡¥äº§å“ä¸åŒç±»å®¢ç¾¤å¸¸ä¸€èµ·è´­ä¹°å“ç±»"},
    {"section_title": "å—ä¼—é—®é¢˜", "search_query_template": "{keyword} common questions faq", "writing_instruction_template": "Top20 å¸¸è§é—®é¢˜ä¸å…³æ³¨ç‚¹"},
    {"section_title": "å—ä¼—åé¦ˆ", "search_query_template": "{keyword} reviews complaints pain points", "writing_instruction_template": "åŒºåˆ†æ­£å‘/è´Ÿå‘ï¼Œåˆ†æè§£å†³è·¯å¾„"},
    {"section_title": "äº§å“è®¤è¯", "search_query_template": "{keyword} certifications regulations", "writing_instruction_template": "å‡ºå£è®¤è¯ï¼ˆFDA/CE/RoHS ç­‰ï¼‰ä¸è´¹ç”¨å‘¨æœŸï¼ˆè‹¥é€‚ç”¨ï¼‰"},
    {"section_title": "é£é™©æŠŠæ§", "search_query_template": "{keyword} safety risks quality control", "writing_instruction_template": "ææ–™/ç»“æ„/åŠŸèƒ½/å®‰å…¨/ä¾›åº”é“¾é£é™©ä¸å¯¹ç­–"},
    {"section_title": "SWOTåˆ†æ", "search_query_template": "{keyword} SWOT analysis", "writing_instruction_template": "SWOT + è¿›å…¥å¯è¡Œæ€§è¯„åˆ† 0â€“10"},
    {"section_title": "KANOæ¨¡å‹åˆ†æ", "search_query_template": "{keyword} must have vs delighter features", "writing_instruction_template": "å¿…å¤‡/æœŸæœ›/é­…åŠ›/æ— å·®å¼‚/åå‘éœ€æ±‚"},
    {"section_title": "ç»†åˆ†å¸‚åœº", "search_query_template": "{keyword} niche markets segments", "writing_instruction_template": "æ¨è 5 ä¸ªç»†åˆ†å¸‚åœºï¼Œé€‰ 1 ä¸ªæ·±æŒ–"},
    {"section_title": "å¼€å‘å»ºè®®", "search_query_template": "{keyword} innovation ideas product improvement", "writing_instruction_template": "ææ–™/å¤–è§‚/é¢œè‰²/åŠŸèƒ½/ç»†èŠ‚äº”ç»´å·®å¼‚åŒ–ï¼Œå¹¶è¯´æ˜ä¸ºä»€ä¹ˆç”¨æˆ·ä¹°å•"}
]


# ============================================
# æ­¥éª¤ 1: SERP API è°ƒç”¨å’Œæ•°æ®æ¸…ç†
# ============================================

async def fetch_serp_data(search_query: str, max_retries: int = 3) -> Dict:
    """
    è°ƒç”¨ SERP API è·å–æœç´¢ç»“æœ
    
    å‚æ•°:
    - search_query: æœç´¢æŸ¥è¯¢
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
    - SERP API è¿”å›çš„ JSON æ•°æ®
    """
    if not SERP_API_KEY:
        error_msg = "SERP_API_KEY æœªé…ç½®æˆ–ä¸ºç©º"
        print(f"âŒ é”™è¯¯: {error_msg}")
        return {}
    
    if is_placeholder_key(SERP_API_KEY):
        error_msg = "SERP_API_KEY æ˜¯å ä½ç¬¦å€¼ï¼Œä¸æ˜¯çœŸå®çš„APIå¯†é’¥"
        print(f"âŒ é”™è¯¯: {error_msg}")
        return {}
    
    params = {
        "api_key": SERP_API_KEY,
        "q": search_query,
        "gl": "us",
        "hl": "en"
    }
    
    for attempt in range(max_retries):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(SERP_API_URL, params=params, timeout=aiohttp.ClientTimeout(total=30)) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data
                    elif response.status == 429:
                        # Rate limitï¼Œç­‰å¾…åé‡è¯•
                        wait_time = (2 ** attempt) * 2
                        print(f"âš ï¸ SERP API é™æµï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                    else:
                        print(f"âš ï¸ SERP API è¿”å›çŠ¶æ€ç : {response.status}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
        except Exception as e:
            print(f"âš ï¸ SERP API è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)
    
    return {}


def clean_serp_data(serp_data: Dict, max_results: int = 8) -> str:
    """
    æ¸…ç† SERP æ•°æ®ï¼Œç”Ÿæˆ cleaned_contextï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
    
    å‚æ•°:
    - serp_data: SERP API è¿”å›çš„åŸå§‹æ•°æ®
    - max_results: æœ€å¤§ç»“æœæ•°
    
    è¿”å›:
    - cleaned_context å­—ç¬¦ä¸²
    """
    def strip_html(html):
        if not html:
            return ''
        return re.sub(r'<[^>]*>', '', str(html)).strip()
    
    cleaned_context = ''
    
    # æå– organic ç»“æœ
    organic_results = serp_data.get('organic', []) or serp_data.get('organic_results', [])
    organic_results = organic_results[:max_results]
    
    for idx, result in enumerate(organic_results):
        title = strip_html(result.get('title', ''))
        snippet = strip_html(result.get('snippet') or result.get('description', ''))
        date = result.get('date') or result.get('published_date', 'N/A')
        link = result.get('link') or result.get('url', '')
        
        cleaned_context += f"[Source {idx + 1}]: {title}\n"
        cleaned_context += f"Content: {snippet}\n"
        cleaned_context += f"Date: {date}\n"
        cleaned_context += f"Link: {link}\n\n"
    
    # æå– People Also Ask
    people_also_ask = serp_data.get('people_also_ask', []) or serp_data.get('related_questions', [])
    if people_also_ask:
        cleaned_context += "\n--- People Also Ask / User Concerns ---\n\n"
        for idx, item in enumerate(people_also_ask):
            question = strip_html(item.get('question') or item.get('title', ''))
            answer = strip_html(item.get('answer') or item.get('snippet', ''))
            cleaned_context += f"Q{idx + 1}: {question}\n"
            if answer:
                cleaned_context += f"A: {answer}\n\n"
    
    return cleaned_context.strip()


# ============================================
# æ­¥éª¤ 2: OpenRouter LLM è°ƒç”¨
# ============================================

async def generate_section_content(
    section_title: str,
    writing_instruction: str,
    cleaned_context: str,
    keyword: str,
    max_retries: int = 3
) -> str:
    """
    ä½¿ç”¨ OpenRouter ç”Ÿæˆç« èŠ‚å†…å®¹
    
    å‚æ•°:
    - section_title: ç« èŠ‚æ ‡é¢˜
    - writing_instruction: å†™ä½œæŒ‡ä»¤
    - cleaned_context: æ¸…ç†åçš„ä¸Šä¸‹æ–‡
    - keyword: å…³é”®è¯
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
    - ç”Ÿæˆçš„ Markdown å†…å®¹
    """
    RULE = f"""
ã€é€šç”¨å†™ä½œåŸåˆ™ â€” å¿…é¡»éµå®ˆã€‘
1. æœ¬æŠ¥å‘Šæ‰€æœ‰å†…å®¹å¿…é¡»ä¸å…³é”®è¯ã€Œ{keyword}ã€ä¿æŒç›´æ¥ä¸å¼ºå…³è”ã€‚
2. ç¦æ­¢æ¨¡å‹æ ¹æ®äº’è”ç½‘å¸¸è¯†ã€æœç´¢ç»“æœã€è¡Œä¸šä¹ æƒ¯è‡ªåŠ¨æ‰©å±•åˆ°å…¶ä»–å“ç±»ã€‚
3. è‹¥æŠ“å–åˆ°çš„ç½‘é¡µä¿¡æ¯åç¦»ã€Œ{keyword}ã€ï¼Œè¿™äº›å†…å®¹å¿…é¡»ä¸¢å¼ƒã€‚
4. æ‰€æœ‰ç»“è®ºå¿…é¡»ä»ã€Œ{keyword}ã€çš„ç‰¹æ€§å‡ºå‘ï¼Œè€Œä¸æ˜¯åŒç±»äº§å“æˆ–ç›¸å…³è¡Œä¸šã€‚
5. å¦‚æ— æ³•ç¡®è®¤æŸä¿¡æ¯æ˜¯å¦å±äºã€Œ{keyword}ã€ï¼Œå¿…é¡»è§†ä¸ºä¸ç›¸å…³å¹¶æ’é™¤ã€‚
"""
    
    system_message = f"""You are an expert Senior Product Manager and Market Analyst with 15 years of experience in Amazon product development. Your task is to write a highly granular, strategic market research report section based ONLY on the provided context.

CURRENT SECTION: ã€ {section_title} ã€‘

SPECIFIC INSTRUCTION FOR THIS SECTION:
{RULE}
{writing_instruction}

GENERAL RULES:
1. Tone: Professional, analytical, objective, and strategic. Avoid generic AI fluff.
2. Format: Use Markdown. Use bullet points, bold text for emphasis, and structured hierarchies.
3. Data: If the Context Data contains numbers (market size, price, percentage), cite them explicitly.
4. Missing Data: If the search context is insufficient, do NOT hallucinate. Instead, provide professional advice on how to get that data (e.g., 'Check Jungle Scout', 'Analyze Competitor Reviews').
5. Language: Output in CHINESE (Simplified), but keep professional terms (like 'CAGR', 'Breastmilk Cooler') in English where appropriate for clarity."""
    
    user_message = cleaned_context
    
    if not OPENROUTER_API_KEY:
        error_msg = "OPENROUTER_API_KEY æœªé…ç½®æˆ–ä¸ºç©º"
        print(f"âŒ é”™è¯¯: {error_msg}")
        return f"âš ï¸ é”™è¯¯: {error_msg}\n\nè¯·æ£€æŸ¥ï¼š\n1. åœ¨ Railway ç¯å¢ƒå˜é‡ä¸­è®¾ç½® OPENROUTER_API_KEY\n2. ç¡®ä¿ä¸æ˜¯å ä½ç¬¦å€¼ï¼ˆå¦‚ __n8n_BLANK_VALUE_ï¼‰\n3. è·å–å¯†é’¥: https://openrouter.ai/keys"
    
    if is_placeholder_key(OPENROUTER_API_KEY):
        error_msg = "OPENROUTER_API_KEY æ˜¯å ä½ç¬¦å€¼ï¼Œä¸æ˜¯çœŸå®çš„APIå¯†é’¥"
        print(f"âŒ é”™è¯¯: {error_msg}")
        return f"âš ï¸ é”™è¯¯: {error_msg}\n\nè¯·æ£€æŸ¥ï¼š\n1. åœ¨ Railway ç¯å¢ƒå˜é‡ä¸­è®¾ç½®çœŸå®çš„ OpenRouter API å¯†é’¥\n2. è·å–å¯†é’¥: https://openrouter.ai/keys\n3. å½“å‰å€¼çœ‹èµ·æ¥æ˜¯å ä½ç¬¦: {OPENROUTER_API_KEY[:50]}..."
    
    payload = {
        "model": "deepseek/deepseek-chat-v3-0324",
        "messages": [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ],
        "temperature": 0.7
    }
    
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    
    for attempt in range(max_retries):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    OPENROUTER_API_URL,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
                        return content
                    elif response.status == 429:
                        wait_time = (2 ** attempt) * 2
                        print(f"âš ï¸ OpenRouter API é™æµï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                    else:
                        error_text = await response.text()
                        print(f"âš ï¸ OpenRouter API è¿”å›çŠ¶æ€ç : {response.status}, é”™è¯¯: {error_text}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
        except Exception as e:
            print(f"âš ï¸ OpenRouter API è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)
    
    return ""


async def execute_research_job(job_id: str, keyword: str):
    """
    æ‰§è¡Œå¸‚åœºè°ƒç ”ä»»åŠ¡ï¼ˆå¼‚æ­¥åå°ä»»åŠ¡ï¼‰
    è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„é•¿æ—¶é—´ä»»åŠ¡ï¼Œéœ€è¦ï¼š
    1. 18 ä¸ªç« èŠ‚çš„ SERP æœç´¢å’Œ LLM ç”Ÿæˆ
    2. Amazon äº§å“æœç´¢å’Œè¯¦æƒ…é¡µè§£æ
    3. Gemini å›¾ç‰‡ç¼–è¾‘
    4. Word æŠ¥å‘Šç”Ÿæˆ
    """
    try:
        job_storage[job_id]["status"] = "running"
        job_storage[job_id]["progress"] = 0.0
        
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œè°ƒç ”ä»»åŠ¡: {job_id}, å…³é”®è¯: {keyword}")
        
        # åˆå§‹åŒ–ç« èŠ‚çŠ¶æ€
        sections_data = []
        for i, task in enumerate(RESEARCH_TASKS):
            sections_data.append({
                "title": task["section_title"],
                "state": "pending",
                "content": None
            })
        job_storage[job_id]["sections"] = sections_data
        
        # æ­¥éª¤ 1: å¹¶å‘æ‰§è¡Œ SERP æœç´¢ï¼ˆé™æµ 5ï¼‰
        serp_semaphore = asyncio.Semaphore(5)
        serp_results = {}
        
        async def fetch_serp_with_limit(idx, task):
            async with serp_semaphore:
                search_query = task["search_query_template"].format(keyword=keyword)
                print(f"  ğŸ“Š [{idx+1}/18] SERP æœç´¢: {task['section_title']}, æŸ¥è¯¢: {search_query}")
                job_storage[job_id]["sections"][idx]["state"] = "serp_fetching"
                result = await fetch_serp_data(search_query)
                print(f"  ğŸ“Š [{idx+1}/18] SERP å“åº”: {len(str(result))} å­—ç¬¦")
                cleaned = clean_serp_data(result)
                print(f"  ğŸ“Š [{idx+1}/18] SERP æ¸…ç†å: {len(cleaned)} å­—ç¬¦")
                serp_results[idx] = cleaned
                job_storage[job_id]["sections"][idx]["state"] = "serp_done"
                job_storage[job_id]["progress"] = (idx + 1) / 18 * 0.3  # SERP å  30% è¿›åº¦
                return cleaned
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ SERP æœç´¢
        serp_tasks = [fetch_serp_with_limit(i, task) for i, task in enumerate(RESEARCH_TASKS)]
        await asyncio.gather(*serp_tasks)
        
        print(f"âœ… SERP æœç´¢å®Œæˆï¼Œå¼€å§‹ LLM ç”Ÿæˆ...")
        
        # æ­¥éª¤ 2: å¹¶å‘æ‰§è¡Œ LLM ç”Ÿæˆï¼ˆé™æµ 3ï¼Œpipeline æ¨¡å¼ï¼‰
        llm_semaphore = asyncio.Semaphore(3)
        llm_results = {}
        
        async def generate_with_limit(idx, task, cleaned_context):
            async with llm_semaphore:
                print(f"  âœï¸ [{idx+1}/18] LLM ç”Ÿæˆ: {task['section_title']}")
                print(f"  âœï¸ [{idx+1}/18] è¾“å…¥ä¸Šä¸‹æ–‡é•¿åº¦: {len(cleaned_context)} å­—ç¬¦")
                job_storage[job_id]["sections"][idx]["state"] = "llm_writing"
                # æ„å»ºå®Œæ•´çš„å†™ä½œæŒ‡ä»¤ï¼ˆåŒ…å« RULEï¼‰
                keyword_rule = f"""
ã€é€šç”¨å†™ä½œåŸåˆ™ â€” å¿…é¡»éµå®ˆã€‘
1. æœ¬æŠ¥å‘Šæ‰€æœ‰å†…å®¹å¿…é¡»ä¸å…³é”®è¯ã€Œ{keyword}ã€ä¿æŒç›´æ¥ä¸å¼ºå…³è”ã€‚
2. ç¦æ­¢æ¨¡å‹æ ¹æ®äº’è”ç½‘å¸¸è¯†ã€æœç´¢ç»“æœã€è¡Œä¸šä¹ æƒ¯è‡ªåŠ¨æ‰©å±•åˆ°å…¶ä»–å“ç±»ã€‚
3. è‹¥æŠ“å–åˆ°çš„ç½‘é¡µä¿¡æ¯åç¦»ã€Œ{keyword}ã€ï¼Œè¿™äº›å†…å®¹å¿…é¡»ä¸¢å¼ƒã€‚
4. æ‰€æœ‰ç»“è®ºå¿…é¡»ä»ã€Œ{keyword}ã€çš„ç‰¹æ€§å‡ºå‘ï¼Œè€Œä¸æ˜¯åŒç±»äº§å“æˆ–ç›¸å…³è¡Œä¸šã€‚
5. å¦‚æ— æ³•ç¡®è®¤æŸä¿¡æ¯æ˜¯å¦å±äºã€Œ{keyword}ã€ï¼Œå¿…é¡»è§†ä¸ºä¸ç›¸å…³å¹¶æ’é™¤ã€‚
"""
                writing_instruction = keyword_rule + "\n" + task["writing_instruction_template"]
                
                # âœ… ä¿®å¤ï¼šæ£€æŸ¥ cleaned_context æ˜¯å¦ä¸ºç©º
                if not cleaned_context or not cleaned_context.strip():
                    error_msg = f"SERP æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå†…å®¹"
                    print(f"  âŒ [{idx+1}/18] é”™è¯¯: {error_msg}")
                    job_storage[job_id]["sections"][idx]["state"] = "failed"
                    job_storage[job_id]["sections"][idx]["error"] = error_msg
                    llm_results[idx] = f"âš ï¸ é”™è¯¯: {error_msg}"
                    return f"âš ï¸ é”™è¯¯: {error_msg}"
                
                content = await generate_section_content(
                    task["section_title"],
                    writing_instruction,
                    cleaned_context,
                    keyword
                )
                print(f"  âœï¸ [{idx+1}/18] LLM è¾“å‡º: {len(content) if content else 0} å­—ç¬¦")
                
                # âœ… ä¿®å¤ï¼šå¦‚æœå†…å®¹ä¸ºç©ºï¼Œæ ‡è®°ä¸ºå¤±è´¥å¹¶é‡è¯•ä¸€æ¬¡
                if not content or not content.strip():
                    print(f"  âš ï¸ [{idx+1}/18] è­¦å‘Š: LLM è¿”å›ç©ºå†…å®¹ï¼Œå°è¯•é‡è¯•...")
                    # é‡è¯•ä¸€æ¬¡
                    content = await generate_section_content(
                        task["section_title"],
                        writing_instruction,
                        cleaned_context,
                        keyword,
                        max_retries=2
                    )
                    
                    if not content or not content.strip():
                        error_msg = f"LLM API è¿”å›ç©ºå†…å®¹ï¼ˆå¯èƒ½æ˜¯ API å¯†é’¥é”™è¯¯ã€ç½‘ç»œé—®é¢˜æˆ–é™æµï¼‰"
                        print(f"  âŒ [{idx+1}/18] é”™è¯¯: {error_msg}")
                        job_storage[job_id]["sections"][idx]["state"] = "failed"
                        job_storage[job_id]["sections"][idx]["error"] = error_msg
                        # è®¾ç½®ä¸€ä¸ªå ä½å†…å®¹ï¼Œè€Œä¸æ˜¯å®Œå…¨ç©º
                        content = f"âš ï¸ é”™è¯¯: {error_msg}\n\nè¯·æ£€æŸ¥ï¼š\n1. OPENROUTER_API_KEY æ˜¯å¦æ­£ç¡®é…ç½®\n2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n3. API æ˜¯å¦è¢«é™æµ"
                    else:
                        print(f"  âœ… [{idx+1}/18] é‡è¯•æˆåŠŸï¼Œç”Ÿæˆå†…å®¹: {len(content)} å­—ç¬¦")
                        job_storage[job_id]["sections"][idx]["state"] = "llm_done"
                else:
                    job_storage[job_id]["sections"][idx]["state"] = "llm_done"
                
                llm_results[idx] = content
                job_storage[job_id]["sections"][idx]["content"] = content
                job_storage[job_id]["progress"] = 0.3 + (idx + 1) / 18 * 0.4  # LLM å  40% è¿›åº¦
                return content
        
        # Pipeline: SERP å®Œæˆåç«‹å³å¼€å§‹ LLMï¼ˆä½†é™æµï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç­‰å¾…æ‰€æœ‰ SERP å®Œæˆåå†å¼€å§‹ LLM
        llm_tasks = [
            generate_with_limit(i, task, serp_results[i])
            for i, task in enumerate(RESEARCH_TASKS)
        ]
        await asyncio.gather(*llm_tasks)
        
        print(f"âœ… LLM ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹ç”ŸæˆæŠ¥å‘Š...")
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ llm_results
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - llm_results é”®æ•°é‡: {len(llm_results)}")
        empty_count = 0
        failed_count = 0
        success_count = 0
        
        for idx, content in llm_results.items():
            content_preview = content[:100] if content else "(ç©º)"
            content_len = len(content) if content else 0
            
            if not content or not content.strip():
                empty_count += 1
                print(f"  âŒ ç« èŠ‚ {idx}: {content_len} å­—ç¬¦ (ç©º), é¢„è§ˆ: {content_preview}")
            elif content.strip().startswith("âš ï¸ é”™è¯¯:"):
                failed_count += 1
                print(f"  âš ï¸ ç« èŠ‚ {idx}: {content_len} å­—ç¬¦ (å¤±è´¥), é¢„è§ˆ: {content_preview}")
            else:
                success_count += 1
                print(f"  âœ… ç« èŠ‚ {idx}: {content_len} å­—ç¬¦, é¢„è§ˆ: {content_preview}")
        
        print(f"ğŸ“Š ç« èŠ‚ç”Ÿæˆç»Ÿè®¡: æˆåŠŸ={success_count}, å¤±è´¥={failed_count}, ç©º={empty_count}, æ€»è®¡={len(llm_results)}")
        
        # âœ… ä¿®å¤ï¼šå¦‚æœæ‰€æœ‰ç« èŠ‚éƒ½å¤±è´¥æˆ–ä¸ºç©ºï¼Œæå‰ç»ˆæ­¢ä»»åŠ¡
        if success_count == 0:
            error_msg = f"æ‰€æœ‰ç« èŠ‚ç”Ÿæˆå¤±è´¥ï¼šæˆåŠŸ={success_count}, å¤±è´¥={failed_count}, ç©º={empty_count}"
            print(f"âŒ {error_msg}")
            job_storage[job_id]["status"] = "failed"
            job_storage[job_id]["error"] = error_msg
            job_storage[job_id]["progress"] = 0.7
            return
        
        # æ­¥éª¤ 3: ç”Ÿæˆ Word æŠ¥å‘Š
        job_storage[job_id]["progress"] = 0.7
        report_path = await generate_word_report(job_id, keyword, llm_results)
        job_storage[job_id]["artifacts"]["report_path"] = report_path
        job_storage[job_id]["progress"] = 0.8
        
        # âœ… ä¿®å¤ï¼šéªŒè¯æŠ¥å‘Šæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰å†…å®¹
        if os.path.exists(report_path):
            file_size = os.path.getsize(report_path)
            print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶å·²ç”Ÿæˆ: {report_path}, å¤§å°: {file_size} å­—èŠ‚")
            if file_size < 1000:  # å¦‚æœæ–‡ä»¶å°äº1KBï¼Œå¯èƒ½åªæœ‰æ ‡é¢˜
                print(f"âš ï¸ è­¦å‘Š: æŠ¥å‘Šæ–‡ä»¶å¾ˆå°ï¼Œå¯èƒ½å†…å®¹ä¸å®Œæ•´")
        else:
            print(f"âŒ é”™è¯¯: æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {report_path}")
        
        # æ­¥éª¤ 4: æå–å¼€å‘å»ºè®®å¹¶ç”Ÿæˆè§†è§‰ prompt
        dev_suggestion = extract_dev_suggestion(llm_results)
        visual_prompt = await generate_visual_prompt(dev_suggestion)
        job_storage[job_id]["artifacts"]["dev_suggestion"] = dev_suggestion
        job_storage[job_id]["artifacts"]["visual_prompt"] = visual_prompt
        
        # æ­¥éª¤ 5: Amazon æœç´¢å’Œè¯¦æƒ…é¡µè§£æ
        amazon_products = await fetch_amazon_products(keyword)
        job_storage[job_id]["artifacts"]["amazon_products"] = amazon_products
        
        # æ­¥éª¤ 6: Gemini å›¾ç‰‡ç¼–è¾‘
        if visual_prompt and amazon_products:
            image_path = await generate_product_image(visual_prompt, amazon_products)
            job_storage[job_id]["artifacts"]["image_path"] = image_path
        
        job_storage[job_id]["status"] = "done"
        job_storage[job_id]["progress"] = 1.0
        print(f"âœ… è°ƒç ”ä»»åŠ¡å®Œæˆ: {job_id}")
        
    except Exception as e:
        job_storage[job_id]["status"] = "failed"
        job_storage[job_id]["error"] = str(e)
        print(f"âŒ è°ƒç ”ä»»åŠ¡å¤±è´¥: {job_id}, é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


# ============================================
# æ­¥éª¤ 3: Word æŠ¥å‘Šç”Ÿæˆ
# ============================================

def markdown_to_html(text: str) -> str:
    """ç®€å•çš„ Markdown è½¬ HTML"""
    if not text:
        return ""
    
    html = text
    # è½¬ä¹‰ HTML ç‰¹æ®Šå­—ç¬¦
    html = html.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    # æ ‡é¢˜
    html = re.sub(r'^### (.*$)', r'<h3>\1</h3>', html, flags=re.MULTILINE)
    html = re.sub(r'^## (.*$)', r'<h2>\1</h2>', html, flags=re.MULTILINE)
    html = re.sub(r'^# (.*$)', r'<h1>\1</h1>', html, flags=re.MULTILINE)
    # åŠ ç²—
    html = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', html)
    # åˆ—è¡¨
    html = re.sub(r'^- (.*$)', r'<li>\1</li>', html, flags=re.MULTILINE)
    # æ¢è¡Œ
    html = html.replace('\n', '<br>')
    
    return html


async def generate_word_report(job_id: str, keyword: str, sections_content: Dict[int, str]) -> str:
    """
    ç”Ÿæˆ Word æŠ¥å‘Šï¼ˆHTML ä¼ªè£…ä¸º .docï¼‰
    
    è¿”å›:
    - æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    html_content = f"""
<html xmlns:o='urn:schemas-microsoft-com:office:office' xmlns:w='urn:schemas-microsoft-com:office:word' xmlns='http://www.w3.org/TR/REC-html40'>
<head><meta charset='utf-8'><title>Market Research Report</title>
<style>
    body {{ font-family: 'Microsoft YaHei', sans-serif; line-height: 1.6; }}
    h1 {{ color: #2E75B6; border-bottom: 2px solid #2E75B6; padding-bottom: 10px; }}
    h2 {{ color: #1F4E79; margin-top: 20px; background-color: #F2F2F2; padding: 5px; }}
    h3 {{ color: #333; }}
    strong {{ color: #C00000; }}
    p {{ margin-bottom: 10px; }}
    li {{ margin-bottom: 5px; }}
    hr {{ border: 0; border-top: 1px solid #ccc; margin: 30px 0; }}
    .error {{ color: #C00000; background-color: #FFE6E6; padding: 10px; margin: 10px 0; }}
</style>
</head><body>
<h1>å…¨ç½‘äº§å“æ·±åº¦è°ƒç ”æŠ¥å‘Š</h1>
<p>å…³é”®è¯: {keyword}</p>
<p>Generated by AI Agent</p>
<hr>
"""
    
    # æŒ‰é¡ºåºæ·»åŠ æ‰€æœ‰ç« èŠ‚
    empty_sections = []
    failed_sections = []
    total_sections = len(RESEARCH_TASKS)
    successful_sections = 0
    
    for i, task in enumerate(RESEARCH_TASKS):
        content = sections_content.get(i, "")
        if content and content.strip():
            # âœ… ä¿®å¤ï¼šæ£€æŸ¥å†…å®¹æ˜¯å¦ä»¥ "âš ï¸ é”™è¯¯:" å¼€å¤´ï¼ˆè¡¨ç¤ºå¤±è´¥ï¼‰
            if content.strip().startswith("âš ï¸ é”™è¯¯:"):
                failed_sections.append({
                    "title": task['section_title'],
                    "error": content.strip()
                })
                html_content += f"<h2>{task['section_title']}</h2>"
                html_content += f"<div class='error'>{markdown_to_html(content)}</div>"
                html_content += "<hr>"
            else:
                html_content += f"<h2>{task['section_title']}</h2>"
                html_content += markdown_to_html(content)
                html_content += "<hr>"
                successful_sections += 1
        else:
            empty_sections.append(task['section_title'])
            print(f"âš ï¸ è­¦å‘Š: ç« èŠ‚ '{task['section_title']}' å†…å®¹ä¸ºç©º")
    
    # âœ… ä¿®å¤ï¼šå¦‚æœæœ‰å¤±è´¥æˆ–ç©ºç« èŠ‚ï¼Œæ·»åŠ è¯¦ç»†çš„é”™è¯¯æç¤º
    if empty_sections or failed_sections:
        html_content += f"<div class='error'><h2>âš ï¸ æŠ¥å‘Šç”Ÿæˆè­¦å‘Š</h2>"
        html_content += f"<p><strong>æˆåŠŸç”Ÿæˆç« èŠ‚: {successful_sections}/{total_sections}</strong></p>"
        
        if failed_sections:
            html_content += f"<p><strong>å¤±è´¥çš„ç« èŠ‚ ({len(failed_sections)} ä¸ª):</strong></p><ul>"
            for section in failed_sections:
                html_content += f"<li><strong>{section['title']}</strong>: {section['error']}</li>"
            html_content += "</ul>"
        
        if empty_sections:
            html_content += f"<p><strong>å†…å®¹ä¸ºç©ºçš„ç« èŠ‚ ({len(empty_sections)} ä¸ª):</strong></p><ul>"
            for section_title in empty_sections:
                html_content += f"<li>{section_title}</li>"
            html_content += "</ul>"
        
        html_content += "<p><strong>è¯·æ£€æŸ¥ï¼š</strong></p><ul>"
        html_content += "<li>SERP_API_KEY æ˜¯å¦æ­£ç¡®é…ç½®ï¼ˆç¯å¢ƒå˜é‡ SERP_API_KEYï¼‰</li>"
        html_content += "<li>OPENROUTER_API_KEY æ˜¯å¦æ­£ç¡®é…ç½®ï¼ˆç¯å¢ƒå˜é‡ OPENROUTER_API_KEYï¼‰</li>"
        html_content += "<li>ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸</li>"
        html_content += "<li>API æ˜¯å¦è¢«é™æµï¼ˆæŸ¥çœ‹åç«¯æ—¥å¿—ï¼‰</li>"
        html_content += "<li>API å¯†é’¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„é¢åº¦</li>"
        html_content += "</ul></div>"
    
    # âœ… ä¿®å¤ï¼šå¦‚æœæ‰€æœ‰ç« èŠ‚éƒ½å¤±è´¥æˆ–ä¸ºç©ºï¼Œæ·»åŠ ä¸¥é‡è­¦å‘Š
    if successful_sections == 0:
        html_content += f"<div class='error' style='background-color: #FFE6E6; border: 2px solid #C00000; padding: 20px; margin: 20px 0;'>"
        html_content += f"<h2 style='color: #C00000;'>âŒ ä¸¥é‡é”™è¯¯ï¼šæŠ¥å‘Šç”Ÿæˆå¤±è´¥</h2>"
        html_content += f"<p>æ‰€æœ‰ {total_sections} ä¸ªç« èŠ‚éƒ½æœªèƒ½æˆåŠŸç”Ÿæˆå†…å®¹ã€‚è¿™é€šå¸¸æ„å‘³ç€ï¼š</p>"
        html_content += "<ol>"
        html_content += "<li><strong>API å¯†é’¥æœªé…ç½®æˆ–é”™è¯¯</strong>ï¼šè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ OPENROUTER_API_KEY å’Œ SERP_API_KEY</li>"
        html_content += "<li><strong>API è°ƒç”¨å¤±è´¥</strong>ï¼šè¯·æŸ¥çœ‹åç«¯æ—¥å¿—äº†è§£è¯¦ç»†é”™è¯¯ä¿¡æ¯</li>"
        html_content += "<li><strong>ç½‘ç»œé—®é¢˜</strong>ï¼šåç«¯æ— æ³•è®¿é—® OpenRouter æˆ– SerpAPI</li>"
        html_content += "<li><strong>API é™æµ</strong>ï¼šè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•</li>"
        html_content += "</ol>"
        html_content += "<p><strong>å»ºè®®æ“ä½œï¼š</strong></p>"
        html_content += "<ol>"
        html_content += "<li>æ£€æŸ¥åç«¯æ—¥å¿—ï¼ˆRailway/æœåŠ¡å™¨æ—¥å¿—ï¼‰</li>"
        html_content += "<li>éªŒè¯ API å¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®</li>"
        html_content += "<li>æ£€æŸ¥ API è´¦æˆ·ä½™é¢å’Œé™åˆ¶</li>"
        html_content += "<li>è”ç³»æŠ€æœ¯æ”¯æŒ</li>"
        html_content += "</ol>"
        html_content += "</div>"
    
    html_content += "</body></html>"
    
    # ä¿å­˜æ–‡ä»¶
    os.makedirs(f"/tmp/research_jobs/{job_id}", exist_ok=True)
    report_path = f"/tmp/research_jobs/{job_id}/Market_Research_Report.doc"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    return report_path


# ============================================
# æ­¥éª¤ 4: å¼€å‘å»ºè®®æå–å’Œè§†è§‰ prompt ç”Ÿæˆ
# ============================================

def extract_dev_suggestion(sections_content: Dict[int, str]) -> str:
    """
    æå–"å¼€å‘å»ºè®®"ç« èŠ‚å†…å®¹
    """
    # ä¼˜å…ˆæŸ¥æ‰¾æ ‡é¢˜åŒ…å«"å¼€å‘å»ºè®®"çš„ç« èŠ‚
    for i, task in enumerate(RESEARCH_TASKS):
        if "å¼€å‘å»ºè®®" in task["section_title"]:
            return sections_content.get(i, "")
    
    # å…¶æ¬¡æŸ¥æ‰¾å†…å®¹åŒ…å«"ã€å¼€å‘å»ºè®®ã€‘"çš„ç« èŠ‚
    for i, content in sections_content.items():
        if "ã€å¼€å‘å»ºè®®ã€‘" in content:
            return content
    
    # æœ€åå–æœ€åä¸€ç« ï¼ˆé€šå¸¸æ˜¯å¼€å‘å»ºè®®ï¼‰
    if sections_content:
        last_idx = max(sections_content.keys())
        return sections_content.get(last_idx, "")
    
    return ""


async def generate_visual_prompt(dev_suggestion: str, max_retries: int = 3) -> str:
    """
    ç”Ÿæˆè‹±æ–‡è§†è§‰ promptï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
    """
    system_message = """You are an expert AI Product Design Prompter for Stable Diffusion (Flux/SDXL). 

### Your Task:
Transform the user's "Development Suggestions" (text) into a set of **visual keywords (English)** for an AI Image Generator.

### Output Requirement:
Output **ONLY** the English Prompt string. Do not output explanations.

### Output Format (Strictly follow this structure):
(Best quality, 8k, masterpiece, product photography), [Subject: Smart Breastmilk Cooler], [Key Features: LED screen, magnetic latch, modular], [Material: Matte plastic, Cooling Gel], [Colors], [Lighting: Studio soft box], [Angle: Front view or Open view]"""
    
    payload = {
        "model": "deepseek/deepseek-chat-v3-0324",
        "messages": [
            {"role": "system", "content": system_message},
            {"role": "user", "content": dev_suggestion}
        ],
        "temperature": 0.7
    }
    
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    
    for attempt in range(max_retries):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    OPENROUTER_API_URL,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        prompt = data.get('choices', [{}])[0].get('message', {}).get('content', '').strip()
                        return prompt
                    elif response.status == 429:
                        await asyncio.sleep((2 ** attempt) * 2)
                    else:
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
        except Exception as e:
            print(f"âš ï¸ è§†è§‰ prompt ç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)
    
    return ""


# ============================================
# æ­¥éª¤ 5: Amazon æœç´¢å’Œè¯¦æƒ…é¡µè§£æ
# ============================================

async def fetch_amazon_products(keyword: str) -> List[Dict]:
    """
    æœç´¢ Amazon å¹¶è·å– Top3 è‡ªç„¶ä½ ASINï¼Œç„¶åè§£æè¯¦æƒ…é¡µ
    """
    # Amazon æœç´¢
    search_url = f"https://www.amazon.com/s?k={quote(keyword)}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.9',
        'Referer': 'https://www.amazon.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'Sec-Ch-Ua-Mobile': '?0',
        'Sec-Ch-Ua-Platform': '"Windows"'
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(search_url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as response:
                if response.status == 200:
                    html = await response.text()
                    # æå– Top3 è‡ªç„¶ä½ ASINï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
                    top3_asins = extract_top3_natural_asins(html)
                    
                    # å¹¶å‘è·å–è¯¦æƒ…é¡µ
                    products = []
                    async with asyncio.Semaphore(2):  # é™æµ 2
                        for asin in top3_asins:
                            product = await fetch_amazon_product_detail(asin)
                            if product:
                                products.append(product)
                    
                    return products
    except Exception as e:
        print(f"âš ï¸ Amazon æœç´¢å¤±è´¥: {str(e)}")
    
    return []


def extract_top3_natural_asins(html: str) -> List[str]:
    """
    ä» Amazon æœç´¢é¡µæå– Top3 è‡ªç„¶ä½ ASINï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
    """
    natural_items = []
    
    # åŒ¹é… search-result å—
    blocks = re.finditer(r'<div[^>]+data-component-type="s-search-result"[^>]*>', html, re.IGNORECASE)
    
    for match in blocks:
        open_tag = match.group(0)
        block_start = match.start()
        block = html[block_start:block_start + 2000]
        
        # å¿…é¡»æ˜¯ listitem
        if not re.search(r'role="listitem"', open_tag, re.IGNORECASE):
            continue
        
        # æå– ASIN
        asin_match = re.search(r'data-asin="(B0[A-Z0-9]{9})"', open_tag, re.IGNORECASE)
        if not asin_match:
            continue
        asin = asin_match.group(1)
        
        # æå– index
        index_match = re.search(r'data-index="(\d+)"', open_tag, re.IGNORECASE)
        if not index_match:
            continue
        index = int(index_match.group(1))
        
        # æ’é™¤ Sponsored
        if re.search(r'Sponsored|s-sponsored-label-text|puis-sponsored-label', block, re.IGNORECASE):
            continue
        
        natural_items.append({"asin": asin, "index": index})
    
    # æ’åºå¹¶å– Top3
    natural_items.sort(key=lambda x: x["index"])
    return [item["asin"] for item in natural_items[:3]]


async def fetch_amazon_product_detail(asin: str) -> Optional[Dict]:
    """
    è·å– Amazon äº§å“è¯¦æƒ…é¡µå¹¶è§£æï¼ˆå‚è€ƒ n8n Python ä»£ç ï¼‰
    """
    url = f"https://www.amazon.com/dp/{asin}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.9',
        'Referer': 'https://www.amazon.com/',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as response:
                if response.status == 200:
                    html = await response.text()
                    return parse_amazon_detail_page(html, asin)
    except Exception as e:
        print(f"âš ï¸ è·å– ASIN {asin} è¯¦æƒ…å¤±è´¥: {str(e)}")
    
    return None


def parse_amazon_detail_page(html: str, asin: str) -> Dict:
    """
    è§£æ Amazon è¯¦æƒ…é¡µï¼ˆå‚è€ƒ n8n Python ä»£ç é€»è¾‘ï¼‰
    """
    # è¿™é‡Œéœ€è¦å®ç°å®Œæ•´çš„è§£æé€»è¾‘ï¼ˆå‚è€ƒä½ æä¾›çš„ Python ä»£ç ï¼‰
    # ç”±äºä»£ç å¾ˆé•¿ï¼Œæˆ‘å…ˆå®ç°åŸºç¡€ç‰ˆæœ¬
    
    def html_unescape(text):
        if not text:
            return text
        replacements = {
            "&quot;": '"', "&#34;": '"', "&apos;": "'", "&#39;": "'",
            "&amp;": "&", "&lt;": "<", "&gt;": ">", "&nbsp;": " "
        }
        for k, v in replacements.items():
            text = text.replace(k, v)
        text = re.sub(r'&#(\d+);', lambda m: chr(int(m.group(1))), text)
        return text
    
    def clean(text):
        if not text:
            return ""
        text = html_unescape(text)
        text = re.sub(r'<(style|script)[^>]*>.*?</\1>', ' ', text, flags=re.DOTALL)
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'[\u200e\u200f]', '', text)
        text = text.replace('&nbsp;', ' ')
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def parse_dimensions_complex(text):
        if not text:
            return None
        text_lower = text.lower()
        strict_match = re.search(r'(\d+(?:\.\d+)?)\s*[xX]\s*(\d+(?:\.\d+)?)\s*[xX]\s*(\d+(?:\.\d+)?)', text)
        dims = []
        if strict_match:
            dims = [float(strict_match.group(1)), float(strict_match.group(2)), float(strict_match.group(3))]
        else:
            numbers = re.findall(r'(\d+(?:\.\d+)?)', text)
            if len(numbers) >= 3:
                dims = [float(numbers[0]), float(numbers[1]), float(numbers[2])]
        
        if len(dims) == 3:
            try:
                if 'cm' in text_lower or 'centimeters' in text_lower:
                    dims = [d / 2.54 for d in dims]
                elif 'mm' in text_lower or 'millimeters' in text_lower:
                    dims = [d / 25.4 for d in dims]
                return sorted(dims, reverse=True)
            except:
                pass
        return None
    
    def extract_dimensions_and_weight_from_text(html_text):
        clean_text = clean(html_text)
        found_dims = None
        found_raw_text = ""
        found_weight = 0.0
        found_weight_unit = ""
        
        keys = ["Product Dimensions", "Package Dimensions", "Item Dimensions", "Dimensions"]
        for key in keys:
            pattern = rf'{key}\s*[:\-]?\s*(\d+(?:\.\d+)?\s*[xX]\s*\d+(?:\.\d+)?\s*[xX]\s*\d+(?:\.\d+)?\s*[a-zA-Z]*)'
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                raw_val = match.group(1)
                parsed = parse_dimensions_complex(raw_val)
                if parsed:
                    found_dims = parsed
                    found_raw_text = raw_val
                    start_pos = match.end()
                    nearby_text = clean_text[start_pos:start_pos + 50]
                    w_match = re.search(r'[;,\s]\s*(\d+(?:\.\d+)?)\s*(pounds?|lbs?|ounces?|oz|grams?|g|kg|kilograms?)', nearby_text, re.IGNORECASE)
                    if w_match:
                        found_weight = float(w_match.group(1))
                        found_weight_unit = w_match.group(2).lower()
                    break
        
        return found_dims, found_raw_text, found_weight, found_weight_unit
    
    def extract_weight_standalone(html_text):
        clean_text = clean(html_text)
        keys = ["Item Weight", "Product Weight", "Shipping Weight"]
        for key in keys:
            pattern = rf'{key}\s*[:\-]?\s*(\d+(?:\.\d+)?)\s*(pounds?|lbs?|ounces?|oz|grams?|g|kg|kilograms?)'
            match = re.search(pattern, clean_text, re.IGNORECASE)
            if match:
                return float(match.group(1)), match.group(2).lower()
        return 0.0, ""
    
    # æå–å°ºå¯¸å’Œé‡é‡
    t_dims, t_raw_text, t_weight, t_w_unit = extract_dimensions_and_weight_from_text(html)
    
    if t_dims:
        L, W, H = t_dims
        raw_dim_text = t_raw_text
    else:
        L, W, H = 0.0, 0.0, 0.0
        raw_dim_text = "NOT_FOUND_IN_TEXT"
    
    if t_weight > 0:
        w_val, w_unit = t_weight, t_w_unit
    else:
        w_val, w_unit = extract_weight_standalone(html)
    
    # æå–å…¶ä»–ä¿¡æ¯
    asin_match = re.search(r'<input[^>]+id="ASIN"[^>]+value="([^"]+)"', html, re.IGNORECASE)
    extracted_asin = asin_match.group(1) if asin_match else asin
    
    main_image_match = re.search(r'"hiRes":"([^"]+)"', html)
    main_image = main_image_match.group(1) if main_image_match else "IMAGE_NOT_FOUND"
    
    price_match = re.search(r'class="a-price-whole">\s*([\d,]+)(?:<[^>]+>)*class="a-price-fraction">\s*(\d+)', html)
    price = 0.0
    if price_match:
        try:
            price = float(f"{price_match.group(1).replace(',', '')}.{price_match.group(2)}")
        except:
            pass
    
    clean_text = clean(html)
    bsr_match = re.search(r'#[\d,]+\s+in\s+([^(<]+?)(?:\(|$)', clean_text, re.IGNORECASE)
    bsr_category = bsr_match.group(1).strip().replace("&", "and") if bsr_match else ""
    bsr_category = re.sub(r'\s+', ' ', bsr_category)
    
    # FBA è®¡ç®—
    weight_lb = w_val
    if "oz" in w_unit or "ounce" in w_unit:
        weight_lb = w_val / 16
    elif "kg" in w_unit or "kilo" in w_unit:
        weight_lb = w_val * 2.20462
    elif "g" in w_unit and "k" not in w_unit:
        weight_lb = w_val * 0.00220462
    
    dim_weight = (L * W * H) / 139
    ship_weight = max(weight_lb, dim_weight)
    girth = 2 * (W + H)
    
    fba_tier = "æœªåˆ†ç±»"
    if ship_weight == 0 and L == 0:
        fba_tier = "æ•°æ®ç¼ºå¤±"
    elif weight_lb <= 1 and L <= 15 and W <= 12 and H <= 0.75:
        fba_tier = "å°å·æ ‡å‡†å°ºå¯¸"
    elif weight_lb <= 20 and L <= 18 and W <= 14 and H <= 8:
        fba_tier = "å¤§å·æ ‡å‡†å°ºå¯¸"
    elif weight_lb <= 50 and (L + girth) <= 130:
        fba_tier = "å¤§å·å¤§ä»¶"
    else:
        fba_tier = "è¶…å¤§ä»¶"
    
    return {
        "asin": extracted_asin,
        "price": price,
        "bsr_category": bsr_category,
        "mainImage": main_image,
        "dimensions": {"length": round(L, 2), "width": round(W, 2), "height": round(H, 2)},
        "weights": {
            "actual_value": w_val,
            "actual_unit": w_unit,
            "shippingWeightLb": round(ship_weight, 2)
        },
        "fba_tier": fba_tier,
        "_debug": {
            "raw_dim_text_found": raw_dim_text
        }
    }


# ============================================
# æ­¥éª¤ 6: Gemini å›¾ç‰‡ç¼–è¾‘
# ============================================

async def generate_product_image(visual_prompt: str, amazon_products: List[Dict]) -> Optional[str]:
    """
    ä½¿ç”¨ Gemini ç¼–è¾‘å›¾ç‰‡
    """
    try:
        import google.generativeai as genai
        
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-pro')
        
        # ä¸‹è½½å‚è€ƒå›¾ç‰‡
        async with aiohttp.ClientSession() as session:
            async with session.get(REFERENCE_IMAGE_URL) as response:
                if response.status == 200:
                    reference_image_bytes = await response.read()
                    
                    # æ„å»º promptï¼ˆå‚è€ƒ n8n é€»è¾‘ï¼‰
                    prompt = f"è¯·ä½ å‚è€ƒå›¾ä¸­çš„äº§å“ï¼Œæ ¹æ®æˆ‘ä»¬çš„subjectä¸­æ˜¾ç¤ºçš„åç§°ï¼ŒæŠŠkey featureä»¥å›¾ä¸­çš„äº§å“ä¸ºåŸºç¡€è¿›è¡Œä½œç”» {visual_prompt}"
                    
                    # è°ƒç”¨ Geminiï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™… API è°ƒæ•´ï¼‰
                    # æ³¨æ„ï¼šGemini Image Edit API å¯èƒ½éœ€è¦ä¸åŒçš„è°ƒç”¨æ–¹å¼
                    # è¿™é‡Œå…ˆè¿”å›å ä½ç¬¦
                    print(f"âš ï¸ Gemini å›¾ç‰‡ç¼–è¾‘åŠŸèƒ½å¾…å®ç°ï¼ˆéœ€è¦ç¡®è®¤ API è°ƒç”¨æ–¹å¼ï¼‰")
                    return None
    except Exception as e:
        print(f"âš ï¸ Gemini å›¾ç‰‡ç¼–è¾‘å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    
    return None


# æ·»åŠ  RULE å¸¸é‡ï¼ˆç”¨äºç”Ÿæˆç« èŠ‚å†…å®¹ï¼‰
RULE = """
ã€é€šç”¨å†™ä½œåŸåˆ™ â€” å¿…é¡»éµå®ˆã€‘
1. æœ¬æŠ¥å‘Šæ‰€æœ‰å†…å®¹å¿…é¡»ä¸å…³é”®è¯ä¿æŒç›´æ¥ä¸å¼ºå…³è”ã€‚
2. ç¦æ­¢æ¨¡å‹æ ¹æ®äº’è”ç½‘å¸¸è¯†ã€æœç´¢ç»“æœã€è¡Œä¸šä¹ æƒ¯è‡ªåŠ¨æ‰©å±•åˆ°å…¶ä»–å“ç±»ã€‚
3. è‹¥æŠ“å–åˆ°çš„ç½‘é¡µä¿¡æ¯åç¦»å…³é”®è¯ï¼Œè¿™äº›å†…å®¹å¿…é¡»ä¸¢å¼ƒã€‚
4. æ‰€æœ‰ç»“è®ºå¿…é¡»ä»å…³é”®è¯çš„ç‰¹æ€§å‡ºå‘ï¼Œè€Œä¸æ˜¯åŒç±»äº§å“æˆ–ç›¸å…³è¡Œä¸šã€‚
5. å¦‚æ— æ³•ç¡®è®¤æŸä¿¡æ¯æ˜¯å¦å±äºå…³é”®è¯ï¼Œå¿…é¡»è§†ä¸ºä¸ç›¸å…³å¹¶æ’é™¤ã€‚
"""

