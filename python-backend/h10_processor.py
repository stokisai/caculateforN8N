"""
H10 ç«å“åˆ†æå¤„ç†æ¨¡å—
å¤„ç†å¤šä¸ª Excel å·¥ä½œè¡¨çš„æ•°æ®åˆ†æå’Œæ ‡è®°
"""

import pandas as pd
import io
import re
from typing import Dict, Optional, List, Tuple
from fastapi import UploadFile, HTTPException


def extract_keywords_from_column(df: pd.DataFrame, column_name: str) -> set:
    """ä»æŒ‡å®šåˆ—æå–å…³é”®è¯ï¼ˆå»é‡ï¼‰"""
    if column_name not in df.columns:
        return set()
    
    keywords = set()
    for value in df[column_name].dropna():
        if pd.notna(value):
            keywords.add(str(value).strip())
    return keywords


def word_boundary_match(text: str, pattern: str) -> bool:
    """
    å•è¯çº§åŒ¹é…ï¼šç¡®ä¿åŒ¹é…å€¼ä½œä¸ºç‹¬ç«‹è¯å‡ºç°ï¼ˆä¸è¢«å…¶å®ƒå­—æ¯æ•°å­—è¿åœ¨ä¸€èµ·ï¼‰
    
    å‚æ•°:
    - text: è¦æœç´¢çš„æ–‡æœ¬
    - pattern: è¦åŒ¹é…çš„æ¨¡å¼
    
    è¿”å›:
    - bool: æ˜¯å¦åŒ¹é…
    """
    if not text or not pattern:
        return False
    
    # è½¬å°å†™è¿›è¡ŒåŒ¹é…
    text_lower = str(text).lower()
    pattern_lower = str(pattern).lower()
    
    # ä½¿ç”¨å•è¯è¾¹ç•Œæ­£åˆ™è¡¨è¾¾å¼
    # \b è¡¨ç¤ºå•è¯è¾¹ç•Œï¼Œç¡®ä¿åŒ¹é…çš„æ˜¯å®Œæ•´å•è¯
    pattern_regex = r'\b' + re.escape(pattern_lower) + r'\b'
    
    return bool(re.search(pattern_regex, text_lower))


def contains_all_words(text: str, words: List[str]) -> bool:
    """
    æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ‰€æœ‰æŒ‡å®šçš„è¯ï¼ˆå•è¯çº§åŒ¹é…ï¼‰
    
    å‚æ•°:
    - text: è¦æ£€æŸ¥çš„æ–‡æœ¬
    - words: è¦åŒ¹é…çš„è¯åˆ—è¡¨
    
    è¿”å›:
    - bool: æ˜¯å¦åŒ…å«æ‰€æœ‰è¯
    """
    if not text or not words:
        return False
    
    for word in words:
        if not word_boundary_match(text, word):
            return False
    return True


async def process_h10_analysis(
    file_H10åæŸ¥æ€»è¡¨: Optional[UploadFile] = None,
    file_ç«å“1: Optional[UploadFile] = None,
    file_ç«å“2: Optional[UploadFile] = None,
    file_ç«å“3: Optional[UploadFile] = None,
    file_ç«å“4: Optional[UploadFile] = None,
    file_ç«å“5: Optional[UploadFile] = None,
    file_ç«å“6: Optional[UploadFile] = None,
    file_ç«å“7: Optional[UploadFile] = None,
    file_ç«å“8: Optional[UploadFile] = None,
    file_ç«å“9: Optional[UploadFile] = None,
    file_ç«å“10: Optional[UploadFile] = None,
    file_è‡ªèº«ASINåæŸ¥: Optional[UploadFile] = None,
    file_ç«å¯¹ABAçƒ­æœè¯åæŸ¥: Optional[UploadFile] = None,
    file_æ‹“è¯åŸºç¡€è¡¨: Optional[UploadFile] = None,
    folder_files: Optional[List[UploadFile]] = None
):
    """
    H10 ç«å“åˆ†æä¸»å¤„ç†å‡½æ•°
    
    å¤„ç†é€»è¾‘ï¼š
    1. è¯»å–æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶
    2. æ‰§è¡Œä¸‰ä¸ªéƒ¨åˆ†çš„æ ‡è®°é€»è¾‘
    3. è¾“å‡ºå¤„ç†åçš„ H10åæŸ¥æ€»è¡¨
    """
    print("ğŸ” å¼€å§‹ H10 ç«å“åˆ†æå¤„ç†...")
    
    # æ­¥éª¤ 1: æ”¶é›†æ‰€æœ‰æ–‡ä»¶
    files_dict = {}
    file_mapping = {
        "H10åæŸ¥æ€»è¡¨": file_H10åæŸ¥æ€»è¡¨,
        "ç«å“1": file_ç«å“1,
        "ç«å“2": file_ç«å“2,
        "ç«å“3": file_ç«å“3,
        "ç«å“4": file_ç«å“4,
        "ç«å“5": file_ç«å“5,
        "ç«å“6": file_ç«å“6,
        "ç«å“7": file_ç«å“7,
        "ç«å“8": file_ç«å“8,
        "ç«å“9": file_ç«å“9,
        "ç«å“10": file_ç«å“10,
        "è‡ªèº«ASINåæŸ¥": file_è‡ªèº«ASINåæŸ¥,
        "ç«å¯¹ABAçƒ­æœè¯åæŸ¥": file_ç«å¯¹ABAçƒ­æœè¯åæŸ¥,
        "æ‹“è¯åŸºç¡€è¡¨": file_æ‹“è¯åŸºç¡€è¡¨,
    }
    
    # å¤„ç†å•ç‹¬ä¸Šä¼ çš„æ–‡ä»¶
    for key, file in file_mapping.items():
        if file:
            files_dict[key] = file
    
    # å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
    if folder_files:
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶å¤¹ï¼ŒåŒ…å« {len(folder_files)} ä¸ªæ–‡ä»¶")
        # åˆ›å»ºæ–‡ä»¶ååŒ¹é…æ˜ å°„
        name_patterns = {
            "H10åæŸ¥æ€»è¡¨": ["h10", "åæŸ¥æ€»è¡¨", "h10åæŸ¥"],
            "ç«å“1": ["ç«å“1", "ç«å“ 1", "competitor1", "comp1"],
            "ç«å“2": ["ç«å“2", "ç«å“ 2", "competitor2", "comp2"],
            "ç«å“3": ["ç«å“3", "ç«å“ 3", "competitor3", "comp3"],
            "ç«å“4": ["ç«å“4", "ç«å“ 4", "competitor4", "comp4"],
            "ç«å“5": ["ç«å“5", "ç«å“ 5", "competitor5", "comp5"],
            "ç«å“6": ["ç«å“6", "ç«å“ 6", "competitor6", "comp6"],
            "ç«å“7": ["ç«å“7", "ç«å“ 7", "competitor7", "comp7"],
            "ç«å“8": ["ç«å“8", "ç«å“ 8", "competitor8", "comp8"],
            "ç«å“9": ["ç«å“9", "ç«å“ 9", "competitor9", "comp9"],
            "ç«å“10": ["ç«å“10", "ç«å“ 10", "competitor10", "comp10"],
            "è‡ªèº«ASINåæŸ¥": ["è‡ªèº«", "asinåæŸ¥", "è‡ªèº«asin"],
            "ç«å¯¹ABAçƒ­æœè¯åæŸ¥": ["ç«å¯¹", "aba", "çƒ­æœè¯", "å¤šasin"],
            "æ‹“è¯åŸºç¡€è¡¨": ["æ‹“è¯", "åŸºç¡€è¡¨"],
        }
        
        for file in folder_files:
            if not file.filename:
                continue
            filename_lower = file.filename.lower()
            # è‡ªåŠ¨åŒ¹é…æ–‡ä»¶å
            matched = False
            for key, patterns in name_patterns.items():
                if any(pattern in filename_lower for pattern in patterns):
                    if key not in files_dict:  # å¦‚æœè¿˜æ²¡è¢«å•ç‹¬ä¸Šä¼ è¦†ç›–
                        files_dict[key] = file
                        print(f"  âœ… åŒ¹é…æ–‡ä»¶: {file.filename} -> {key}")
                        matched = True
                        break
            if not matched:
                print(f"  âš ï¸ æœªåŒ¹é…æ–‡ä»¶: {file.filename}")
    
    # éªŒè¯å¿…éœ€æ–‡ä»¶
    if "H10åæŸ¥æ€»è¡¨" not in files_dict:
        raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…éœ€æ–‡ä»¶ï¼šH10åæŸ¥æ€»è¡¨")
    
    print(f"ğŸ“Š å·²æ”¶é›† {len(files_dict)} ä¸ªæ–‡ä»¶: {list(files_dict.keys())}")
    
    # æ­¥éª¤ 2: è¯»å–æ‰€æœ‰ Excel æ–‡ä»¶
    dataframes = {}
    for key, file in files_dict.items():
        try:
            content = await file.read()
            # å°è¯•è¯»å– Excel æ–‡ä»¶
            excel_file = pd.ExcelFile(io.BytesIO(content))
            
            # å¦‚æœæœ‰å¤šä¸ªå·¥ä½œè¡¨ï¼Œè¯»å–ç¬¬ä¸€ä¸ªï¼ˆæˆ–æ ¹æ®åç§°åŒ¹é…ï¼‰
            if key == "ç«å¯¹ABAçƒ­æœè¯åæŸ¥":
                # âœ… ä¸¥æ ¼æŒ‰ç…§éœ€æ±‚ï¼šæŸ¥æ‰¾"å¤šasinåæŸ¥æµé‡"å·¥ä½œè¡¨ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
                sheet_name = None
                for sheet in excel_file.sheet_names:
                    if sheet == "å¤šasinåæŸ¥æµé‡" or sheet == "å¤šASINåæŸ¥æµé‡":
                        sheet_name = sheet
                        break
                if not sheet_name:
                    # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                    for sheet in excel_file.sheet_names:
                        if "å¤šasin" in sheet or "å¤šASIN" in sheet:
                            sheet_name = sheet
                            break
                if not sheet_name:
                    print(f"  âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°'å¤šasinåæŸ¥æµé‡'å·¥ä½œè¡¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨")
                    sheet_name = excel_file.sheet_names[0]
                dataframes[key] = pd.read_excel(excel_file, sheet_name=sheet_name)
                print(f"  âœ… {key}: è¯»å–å·¥ä½œè¡¨ '{sheet_name}'")
            else:
                # å…¶ä»–æ–‡ä»¶è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                dataframes[key] = pd.read_excel(excel_file, sheet_name=0)
                print(f"  âœ… {key}: è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨")
        except Exception as e:
            print(f"  âŒ è¯»å– {key} å¤±è´¥: {str(e)}")
            raise HTTPException(status_code=400, detail=f"è¯»å–æ–‡ä»¶ {key} å¤±è´¥: {str(e)}")
    
    # æ­¥éª¤ 3: æ‰§è¡Œå¤„ç†é€»è¾‘
    h10_df = dataframes["H10åæŸ¥æ€»è¡¨"].copy()
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šAN åˆ—æ ‡è®°ï¼ˆA-F ä¼˜å…ˆçº§ï¼‰
    h10_df = process_part1_an_column(h10_df, dataframes)
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šAO åˆ—æ ‡è®°ï¼ˆè¯ç±»å‹åˆ†ç±»ï¼‰
    h10_df = process_part2_ao_column(h10_df, dataframes)
    
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šAP åˆ—æ ‡è®°ï¼ˆæµé‡ç­‰çº§ï¼‰
    h10_df = process_part3_ap_column(h10_df)
    
    # æ­¥éª¤ 4: è¾“å‡ºç»“æœ
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        h10_df.to_excel(writer, sheet_name='H10åæŸ¥æ€»è¡¨', index=False)
    output.seek(0)
    
    print("âœ… H10 å¤„ç†å®Œæˆï¼Œå‡†å¤‡è¿”å›æ–‡ä»¶")
    return output


def process_part1_an_column(h10_df: pd.DataFrame, dataframes: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    ç¬¬ä¸€éƒ¨åˆ†ï¼šAN åˆ—æ ‡è®°ï¼ˆA-F ä¼˜å…ˆçº§ï¼‰
    
    ä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼šF > E > D > C > B > A
    """
    print("ğŸ“ ç¬¬ä¸€éƒ¨åˆ†ï¼šå¤„ç† AN åˆ—æ ‡è®°...")
    
    # æŸ¥æ‰¾å…³é”®è¯è¯ç»„åˆ—ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€åˆ—æˆ–åŒ…å«"å…³é”®è¯"çš„åˆ—ï¼‰
    keyword_col = None
    for col in h10_df.columns:
        if "å…³é”®è¯" in str(col) or "è¯ç»„" in str(col):
            keyword_col = col
            break
    if not keyword_col:
        keyword_col = h10_df.columns[0]  # é»˜è®¤ç¬¬ä¸€åˆ—
    
    # åˆå§‹åŒ– AN åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°æœ«å°¾ï¼‰
    if "AN" not in h10_df.columns:
        h10_df["AN"] = ""
    else:
        # å¦‚æœå·²å­˜åœ¨ï¼Œæ¸…ç©ºæˆ–ä¿æŒ
        h10_df["AN"] = h10_df["AN"].fillna("")
    
    # æ”¶é›†å„å·¥ä½œè¡¨çš„å…³é”®è¯
    # 1. è‡ªèº«ASINåæŸ¥
    self_asin_keywords = set()
    if "è‡ªèº«ASINåæŸ¥" in dataframes:
        df = dataframes["è‡ªèº«ASINåæŸ¥"]
        # æŸ¥æ‰¾å…³é”®è¯åˆ—
        for col in df.columns:
            if "å…³é”®è¯" in str(col) or "è¯ç»„" in str(col):
                self_asin_keywords = extract_keywords_from_column(df, col)
                break
    
    # 2. ç«å¯¹ABAçƒ­æœè¯åæŸ¥ - å¤šasinåæŸ¥æµé‡å·¥ä½œè¡¨
    aba_keywords = set()
    if "ç«å¯¹ABAçƒ­æœè¯åæŸ¥" in dataframes:
        df = dataframes["ç«å¯¹ABAçƒ­æœè¯åæŸ¥"]
        for col in df.columns:
            if "å…³é”®è¯" in str(col) or "è¯ç»„" in str(col):
                aba_keywords = extract_keywords_from_column(df, col)
                break
    
    # 3. ç«å“1-10
    competitor_keywords_data = {}  # {keyword: {ç«å“: {å¹¿å‘Šæ’å, è‡ªç„¶æ’å}}}
    for i in range(1, 11):
        key = f"ç«å“{i}"
        if key in dataframes:
            df = dataframes[key]
            # æŸ¥æ‰¾å…³é”®è¯åˆ—ã€å¹¿å‘Šæ’ååˆ—ã€è‡ªç„¶æ’ååˆ—
            keyword_col_comp = None
            ad_rank_col = None
            natural_rank_col = None
            
            for col in df.columns:
                col_str = str(col).lower()
                if "å…³é”®è¯" in col_str or "è¯ç»„" in col_str:
                    keyword_col_comp = col
                elif "å¹¿å‘Š" in col_str and ("æ’å" in col_str or "rank" in col_str):
                    ad_rank_col = col
                elif ("è‡ªç„¶" in col_str or "organic" in col_str) and ("æ’å" in col_str or "rank" in col_str):
                    natural_rank_col = col
            
            if keyword_col_comp:
                for idx, row in df.iterrows():
                    keyword = str(row[keyword_col_comp]).strip() if pd.notna(row[keyword_col_comp]) else ""
                    if keyword:
                        if keyword not in competitor_keywords_data:
                            competitor_keywords_data[keyword] = {}
                        
                        ad_rank = None
                        natural_rank = None
                        
                        if ad_rank_col and pd.notna(row[ad_rank_col]):
                            try:
                                ad_rank = float(row[ad_rank_col])
                            except:
                                pass
                        
                        if natural_rank_col and pd.notna(row[natural_rank_col]):
                            try:
                                natural_rank = float(row[natural_rank_col])
                            except:
                                pass
                        
                        competitor_keywords_data[keyword][key] = {
                            "ad_rank": ad_rank,
                            "natural_rank": natural_rank
                        }
    
    # æ ‡è®° AN åˆ—ï¼ˆä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·éœ€æ±‚ï¼Œä¼˜å…ˆçº§ï¼šF > E > D > C > B > Aï¼‰
    print(f"  æ”¶é›†åˆ°çš„å…³é”®è¯æ•°é‡: è‡ªèº«ASIN={len(self_asin_keywords)}, ABA={len(aba_keywords)}, ç«å“={len(competitor_keywords_data)}")
    
    for idx, row in h10_df.iterrows():
        keyword = str(row[keyword_col]).strip() if pd.notna(row[keyword_col]) else ""
        if not keyword:
            h10_df.at[idx, "AN"] = "A"
            continue
        
        # âœ… ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·éœ€æ±‚çš„ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥
        
        # è§„åˆ™ 1: å¦‚æœå…³é”®è¯åœ¨è‡ªèº«ASINåæŸ¥ä¸­å‡ºç° -> Fï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if keyword in self_asin_keywords:
            h10_df.at[idx, "AN"] = "F"
            continue
        
        # è§„åˆ™ 2: å¦‚æœå…³é”®è¯åœ¨ç«å¯¹ABAçƒ­æœè¯åæŸ¥ä¸­å‡ºç° -> E
        if keyword in aba_keywords:
            h10_df.at[idx, "AN"] = "E"
            continue
        
        # è§„åˆ™ 3-6: æ£€æŸ¥ç«å“æ•°æ®
        if keyword in competitor_keywords_data:
            comp_data = competitor_keywords_data[keyword]
            
            # âœ… ä¿®å¤ï¼šæ”¶é›†æ‰€æœ‰æ»¡è¶³çš„æ¡ä»¶ï¼Œç„¶åé€‰æ‹©æœ€é«˜ä¼˜å…ˆçº§ï¼ˆD > C > Bï¼‰
            has_d = False  # è§„åˆ™ 3: å¹¿å‘Šæ’å<=20 ä¸” è‡ªç„¶æ’å<=20ï¼ˆä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³ï¼‰
            has_c = False  # è§„åˆ™ 4: ä»…è‡ªç„¶æ’å<=20ï¼ˆå¹¿å‘Šæ’å>20æˆ–æ— å€¼ï¼Œä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³ï¼‰
            has_b = False  # è§„åˆ™ 5: ä»…å¹¿å‘Šæ’å<=20ï¼ˆè‡ªç„¶æ’å>20æˆ–æ— å€¼ï¼Œä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³ï¼‰
            
            # éå†æ‰€æœ‰ç«å“ï¼Œæ£€æŸ¥æ˜¯å¦æ»¡è¶³D/C/Bæ¡ä»¶ï¼ˆä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³å³å¯ï¼‰
            # âœ… ä¿®å¤ï¼šéœ€è¦æ£€æŸ¥æ‰€æœ‰ç«å“ï¼Œåˆ†åˆ«åˆ¤æ–­æ¯ä¸ªç«å“æ˜¯å¦æ»¡è¶³D/C/Bæ¡ä»¶
            for comp_name, ranks in comp_data.items():
                ad_rank = ranks.get("ad_rank")
                natural_rank = ranks.get("natural_rank")
                
                # è§„åˆ™ 3: å¹¿å‘Šæ’å<=20 ä¸” è‡ªç„¶æ’å<=20ï¼ˆä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³å³å¯ï¼‰
                if (ad_rank is not None and ad_rank <= 20) and (natural_rank is not None and natural_rank <= 20):
                    has_d = True
                
                # è§„åˆ™ 4: ä»…è‡ªç„¶æ’å<=20ï¼ˆå¹¿å‘Šæ’å>20æˆ–æ— å€¼ï¼Œä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³å³å¯ï¼‰
                # æ³¨æ„ï¼šä½¿ç”¨ç‹¬ç«‹çš„ifï¼Œå› ä¸ºéœ€è¦æ£€æŸ¥æ‰€æœ‰ç«å“ï¼Œçœ‹æ˜¯å¦æœ‰ä»»æ„ä¸€ä¸ªæ»¡è¶³Cæ¡ä»¶
                if (natural_rank is not None and natural_rank <= 20) and (ad_rank is None or ad_rank > 20):
                    has_c = True
                
                # è§„åˆ™ 5: ä»…å¹¿å‘Šæ’å<=20ï¼ˆè‡ªç„¶æ’å>20æˆ–æ— å€¼ï¼Œä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³å³å¯ï¼‰
                # æ³¨æ„ï¼šä½¿ç”¨ç‹¬ç«‹çš„ifï¼Œå› ä¸ºéœ€è¦æ£€æŸ¥æ‰€æœ‰ç«å“ï¼Œçœ‹æ˜¯å¦æœ‰ä»»æ„ä¸€ä¸ªæ»¡è¶³Bæ¡ä»¶
                if (ad_rank is not None and ad_rank <= 20) and (natural_rank is None or natural_rank > 20):
                    has_b = True
            
            # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©æœ€é«˜æ ‡è®°ï¼ˆD > C > Bï¼‰
            if has_d:
                h10_df.at[idx, "AN"] = "D"
            elif has_c:
                h10_df.at[idx, "AN"] = "C"
            elif has_b:
                h10_df.at[idx, "AN"] = "B"
            else:
                # è§„åˆ™ 6: å¦‚æœæ‰€æœ‰ç«å“çš„å¹¿å‘Šæ’åå’Œè‡ªç„¶æ’åéƒ½>20æˆ–æ— å€¼ï¼ˆæ‰€æœ‰ç«å“éƒ½è¦æ»¡è¶³è¿™ä¸ªæ¡ä»¶ï¼‰-> A
                all_comp_high = True
                for comp_name, ranks in comp_data.items():
                    ad_rank = ranks.get("ad_rank")
                    natural_rank = ranks.get("natural_rank")
                    # å¯¹äºæ¯ä¸ªç«å“ï¼Œå¹¿å‘Šæ’åå’Œè‡ªç„¶æ’åéƒ½å¿…é¡»>20æˆ–æ— å€¼
                    if (ad_rank is not None and ad_rank <= 20) or (natural_rank is not None and natural_rank <= 20):
                        all_comp_high = False
                        break
                
                if all_comp_high:
                    h10_df.at[idx, "AN"] = "A"
                else:
                    # å¦‚æœæ‰€æœ‰ç«å“éƒ½>20çš„æ¡ä»¶ä¸æ»¡è¶³ï¼Œä½†D/C/Béƒ½ä¸æ»¡è¶³ï¼Œè¯´æ˜æ•°æ®æœ‰é—®é¢˜
                    # æŒ‰ç…§éœ€æ±‚ï¼Œè¿™ç§æƒ…å†µä¸åº”è¯¥å‡ºç°ï¼Œä½†ä¸ºäº†ä¿é™©æ ‡è®°ä¸ºA
                    h10_df.at[idx, "AN"] = "A"
        else:
            # è§„åˆ™ 6: å¦‚æœå…³é”®è¯åœ¨ç«å“1-10ä¸­æœªå‡ºç° -> A
            h10_df.at[idx, "AN"] = "A"
    
    # ç»Ÿè®¡æ ‡è®°ç»“æœ
    mark_counts = h10_df["AN"].value_counts().to_dict()
    print(f"  âœ… AN åˆ—æ ‡è®°å®Œæˆï¼Œå…±å¤„ç† {len(h10_df)} è¡Œï¼Œç»Ÿè®¡: {mark_counts}")
    print(f"  ğŸ“Š ANåˆ—è¯¦ç»†ç»Ÿè®¡: F={mark_counts.get('F', 0)}, E={mark_counts.get('E', 0)}, D={mark_counts.get('D', 0)}, C={mark_counts.get('C', 0)}, B={mark_counts.get('B', 0)}, A={mark_counts.get('A', 0)}")
    return h10_df


def process_part2_ao_column(h10_df: pd.DataFrame, dataframes: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    ç¬¬äºŒéƒ¨åˆ†ï¼šAO åˆ—æ ‡è®°ï¼ˆè¯ç±»å‹åˆ†ç±»ï¼‰
    """
    print("ğŸ“ ç¬¬äºŒéƒ¨åˆ†ï¼šå¤„ç† AO åˆ—æ ‡è®°...")
    
    # åˆå§‹åŒ– AO åˆ—
    if "AO" not in h10_df.columns:
        h10_df.insert(len(h10_df.columns), "AO", "")
    
    # æŸ¥æ‰¾å…³é”®è¯åˆ—
    keyword_col = None
    for col in h10_df.columns:
        if "å…³é”®è¯" in str(col) or "è¯ç»„" in str(col):
            keyword_col = col
            break
    if not keyword_col:
        keyword_col = h10_df.columns[0]
    
    # è¯»å–æ‹“è¯åŸºç¡€è¡¨
    if "æ‹“è¯åŸºç¡€è¡¨" not in dataframes:
        print("  âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æ‹“è¯åŸºç¡€è¡¨ï¼Œè·³è¿‡ AO åˆ—æ ‡è®°")
        return h10_df
    
    tuoci_df = dataframes["æ‹“è¯åŸºç¡€è¡¨"]
    
    # æå–å„åˆ—çš„å€¼ï¼ˆä½œä¸ºå®Œæ•´è¯åŒ¹é…ï¼‰
    col_a_values = []  # Aåˆ—
    col_b_values = []  # Båˆ—
    col_c_values = []  # Cåˆ—
    col_d_values = []  # Dåˆ—
    col_e_values = []  # Eåˆ—
    
    # å‡è®¾åˆ—æŒ‰é¡ºåºï¼šA, B, C, D, E
    if len(tuoci_df.columns) >= 1:
        col_a_values = [str(v).strip() for v in tuoci_df.iloc[:, 0].dropna() if str(v).strip()]
    if len(tuoci_df.columns) >= 2:
        col_b_values = [str(v).strip() for v in tuoci_df.iloc[:, 1].dropna() if str(v).strip()]
    if len(tuoci_df.columns) >= 3:
        col_c_values = [str(v).strip() for v in tuoci_df.iloc[:, 2].dropna() if str(v).strip()]
    if len(tuoci_df.columns) >= 4:
        col_d_values = [str(v).strip() for v in tuoci_df.iloc[:, 3].dropna() if str(v).strip()]
    if len(tuoci_df.columns) >= 5:
        col_e_values = [str(v).strip() for v in tuoci_df.iloc[:, 4].dropna() if str(v).strip()]
    
    print(f"  æ‹“è¯åŸºç¡€è¡¨: Aåˆ—={len(col_a_values)}, Båˆ—={len(col_b_values)}, Cåˆ—={len(col_c_values)}, Dåˆ—={len(col_d_values)}, Eåˆ—={len(col_e_values)}")
    
    # æ ‡è®° AO åˆ—
    for idx, row in h10_df.iterrows():
        keyword = str(row[keyword_col]).strip() if pd.notna(row[keyword_col]) else ""
        if not keyword:
            h10_df.at[idx, "AO"] = "ç›¸å…³è¯"
            continue
        
        mark = None
        
        # âœ… ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·éœ€æ±‚çš„è§„åˆ™é¡ºåºæ£€æŸ¥
        
        # è§„åˆ™ 1: åŒ…å« D åˆ—ä¸­çš„ä»»æ„å•å…ƒæ ¼ä¸­çš„å…¨éƒ¨å€¼ -> ä¸ç›¸å…³è¯ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if any(word_boundary_match(keyword, val) for val in col_d_values):
            mark = "ä¸ç›¸å…³è¯"
        # è§„åˆ™ 2: åŒ…å« A åˆ—ï¼Œä¸åŒ…å« B/C/D/E åˆ— -> å¤§è¯æˆ–æ³›è¯
        elif (any(word_boundary_match(keyword, val) for val in col_a_values) and
              not any(word_boundary_match(keyword, val) for val in col_b_values) and
              not any(word_boundary_match(keyword, val) for val in col_c_values) and
              not any(word_boundary_match(keyword, val) for val in col_d_values) and
              not any(word_boundary_match(keyword, val) for val in col_e_values)):
            mark = "å¤§è¯æˆ–æ³›è¯"
        # è§„åˆ™ 3: åŒ…å« A åˆ—å’Œ B åˆ—ï¼Œä¸åŒ…å« D/E åˆ— -> aç²¾å‡†å±æ€§ç²¾å‡†è¯
        elif (any(word_boundary_match(keyword, val) for val in col_a_values) and
              any(word_boundary_match(keyword, val) for val in col_b_values) and
              not any(word_boundary_match(keyword, val) for val in col_d_values) and
              not any(word_boundary_match(keyword, val) for val in col_e_values)):
            mark = "aç²¾å‡†å±æ€§ç²¾å‡†è¯"
        # è§„åˆ™ 4: åŒ…å« A åˆ—å’Œ C åˆ—ï¼Œä¸åŒ…å« B/D/E åˆ— -> bæ³›å±æ€§ç²¾å‡†è¯
        elif (any(word_boundary_match(keyword, val) for val in col_a_values) and
              any(word_boundary_match(keyword, val) for val in col_c_values) and
              not any(word_boundary_match(keyword, val) for val in col_b_values) and
              not any(word_boundary_match(keyword, val) for val in col_d_values) and
              not any(word_boundary_match(keyword, val) for val in col_e_values)):
            mark = "bæ³›å±æ€§ç²¾å‡†è¯"
        # è§„åˆ™ 5: åŒ…å« B åˆ—æˆ– C åˆ—ï¼Œä¸åŒ…å« A/D/E åˆ— -> ç›¸å…³è¯
        elif ((any(word_boundary_match(keyword, val) for val in col_b_values) or
               any(word_boundary_match(keyword, val) for val in col_c_values)) and
              not any(word_boundary_match(keyword, val) for val in col_a_values) and
              not any(word_boundary_match(keyword, val) for val in col_d_values) and
              not any(word_boundary_match(keyword, val) for val in col_e_values)):
            mark = "ç›¸å…³è¯"
        # è§„åˆ™ 6: ä¸åŒ…å« A/B/C/D/E åˆ— -> ç›¸å…³è¯
        elif (not any(word_boundary_match(keyword, val) for val in col_a_values) and
              not any(word_boundary_match(keyword, val) for val in col_b_values) and
              not any(word_boundary_match(keyword, val) for val in col_c_values) and
              not any(word_boundary_match(keyword, val) for val in col_d_values) and
              not any(word_boundary_match(keyword, val) for val in col_e_values)):
            mark = "ç›¸å…³è¯"
        # è§„åˆ™ 7: åŒ…å« E åˆ— -> å“ç‰Œè¯ï¼ˆæœ€åæ£€æŸ¥ï¼Œä½†ä¼šè¦†ç›–å…¶ä»–æ ‡è®°ï¼‰
        elif any(word_boundary_match(keyword, val) for val in col_e_values):
            mark = "å“ç‰Œè¯"
        else:
            mark = "ç›¸å…³è¯"  # é»˜è®¤
        
        h10_df.at[idx, "AO"] = mark
    
    # ç»Ÿè®¡æ ‡è®°ç»“æœ
    mark_counts = h10_df["AO"].value_counts().to_dict()
    print(f"  âœ… AO åˆ—æ ‡è®°å®Œæˆï¼Œç»Ÿè®¡: {mark_counts}")
    return h10_df


def process_part3_ap_column(h10_df: pd.DataFrame) -> pd.DataFrame:
    """
    ç¬¬ä¸‰éƒ¨åˆ†ï¼šAP åˆ—æ ‡è®°ï¼ˆæµé‡ç­‰çº§ï¼‰
    
    æ ¹æ®æœç´¢é‡ç´¯è®¡ç™¾åˆ†æ¯”æ ‡è®°ï¼š
    - é«˜æµé‡è¯1: 0-40%
    - ä¸­é«˜æµé‡è¯2: 40-70%
    - ä¸­ä½æµé‡è¯3: 70-90%
    - ä½æµé‡è¯4: 90-100%
    """
    print("ğŸ“ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¤„ç† AP åˆ—æ ‡è®°...")
    
    # åˆå§‹åŒ– AP åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°æœ«å°¾ï¼‰
    if "AP" not in h10_df.columns:
        h10_df["AP"] = ""
    else:
        # å¦‚æœå·²å­˜åœ¨ï¼Œæ¸…ç©ºæˆ–ä¿æŒ
        h10_df["AP"] = h10_df["AP"].fillna("")
    
    # æŸ¥æ‰¾æœç´¢é‡åˆ—ï¼ˆDåˆ—ï¼Œç´¢å¼•3ï¼‰
    search_volume_col = None
    if len(h10_df.columns) > 3:
        search_volume_col = h10_df.columns[3]  # Dåˆ—
    else:
        # å°è¯•æŸ¥æ‰¾åŒ…å«"æœç´¢é‡"çš„åˆ—
        for col in h10_df.columns:
            if "æœç´¢é‡" in str(col) or "search" in str(col).lower():
                search_volume_col = col
                break
    
    if not search_volume_col:
        print("  âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æœç´¢é‡åˆ—ï¼ˆDåˆ—ï¼‰ï¼Œè·³è¿‡ AP åˆ—æ ‡è®°")
        return h10_df
    
    # æå–æœç´¢é‡å¹¶è½¬æ¢ä¸ºæ•°å€¼
    search_volumes = []
    for idx, row in h10_df.iterrows():
        value = row[search_volume_col]
        if pd.notna(value):
            try:
                vol = float(value)
                search_volumes.append((idx, vol))
            except:
                search_volumes.append((idx, 0))
        else:
            search_volumes.append((idx, 0))
    
    # æŒ‰æœç´¢é‡é™åºæ’åº
    search_volumes.sort(key=lambda x: x[1], reverse=True)
    
    # è®¡ç®—æ€»æœç´¢é‡
    total_volume = sum(vol for _, vol in search_volumes)
    
    if total_volume == 0:
        print("  âš ï¸ è­¦å‘Š: æ€»æœç´¢é‡ä¸º0ï¼Œè·³è¿‡ AP åˆ—æ ‡è®°")
        return h10_df
    
    # è®¡ç®—ç´¯è®¡ç™¾åˆ†æ¯”å¹¶æ ‡è®°
    cumulative_volume = 0
    current_threshold = 0.4  # 40%
    current_mark = "é«˜æµé‡è¯1"
    
    for idx, vol in search_volumes:
        cumulative_volume += vol
        percentage = cumulative_volume / total_volume
        
        if percentage <= 0.4:
            mark = "é«˜æµé‡è¯1"
        elif percentage <= 0.7:
            mark = "ä¸­é«˜æµé‡è¯2"
        elif percentage <= 0.9:
            mark = "ä¸­ä½æµé‡è¯3"
        else:
            mark = "ä½æµé‡è¯4"
        
        h10_df.at[idx, "AP"] = mark
    
    print(f"  âœ… AP åˆ—æ ‡è®°å®Œæˆï¼Œæ€»æœç´¢é‡: {total_volume:,.0f}")
    return h10_df

