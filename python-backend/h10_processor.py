"""
H10 ç«å“åˆ†æå¤„ç†æ¨¡å—
å¤„ç†å¤šä¸ª Excel å·¥ä½œè¡¨çš„æ•°æ®åˆ†æå’Œæ ‡è®°
"""

import pandas as pd
import io
import re
import json
import time
from typing import Dict, Optional, List, Tuple
from fastapi import UploadFile, HTTPException


def extract_keywords_from_column(df: pd.DataFrame, column_name: str) -> set:
    """ä»æŒ‡å®šåˆ—æå–å…³é”®è¯ï¼ˆå»é‡ï¼‰"""
    if column_name not in df.columns:
        return set()
    
    keywords = set()
    for value in df[column_name].dropna():
        if pd.notna(value):
            keywords.add(str(value).strip())
    return keywords


def word_boundary_match(text: str, pattern: str) -> bool:
    """
    å•è¯çº§åŒ¹é…ï¼šç¡®ä¿åŒ¹é…å€¼ä½œä¸ºç‹¬ç«‹è¯å‡ºç°ï¼ˆä¸è¢«å…¶å®ƒå­—æ¯æ•°å­—è¿åœ¨ä¸€èµ·ï¼‰
    å‚è€ƒç”¨æˆ·ç‰ˆæœ¬å®ç°
    """
    if not text or not pattern:
        return False
    
    # è½¬å°å†™è¿›è¡ŒåŒ¹é…
    text_lower = str(text).lower()
    pattern_lower = str(pattern).strip().lower()
    
    if not pattern_lower:
        return False
    
    # âœ… å‚è€ƒç”¨æˆ·ç‰ˆæœ¬ï¼šä½¿ç”¨ (?<![0-9a-zA-Z]) å’Œ (?![0-9a-zA-Z]) ç¡®ä¿ä¸åœ¨å­—æ¯æ•°å­—ä¹‹é—´
    pattern_regex = rf'(?<![0-9a-zA-Z]){re.escape(pattern_lower)}(?![0-9a-zA-Z])'
    
    return bool(re.search(pattern_regex, text_lower))


def contains_all_words(text: str, words: List[str]) -> bool:
    """
    æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ‰€æœ‰æŒ‡å®šçš„è¯ï¼ˆå•è¯çº§åŒ¹é…ï¼‰
    
    å‚æ•°:
    - text: è¦æ£€æŸ¥çš„æ–‡æœ¬
    - words: è¦åŒ¹é…çš„è¯åˆ—è¡¨
    
    è¿”å›:
    - bool: æ˜¯å¦åŒ…å«æ‰€æœ‰è¯
    """
    if not text or not words:
        return False
    
    for word in words:
        if not word_boundary_match(text, word):
            return False
    return True


async def process_h10_analysis(
    file_H10åæŸ¥æ€»è¡¨: Optional[UploadFile] = None,
    file_ç«å“1: Optional[UploadFile] = None,
    file_ç«å“2: Optional[UploadFile] = None,
    file_ç«å“3: Optional[UploadFile] = None,
    file_ç«å“4: Optional[UploadFile] = None,
    file_ç«å“5: Optional[UploadFile] = None,
    file_ç«å“6: Optional[UploadFile] = None,
    file_ç«å“7: Optional[UploadFile] = None,
    file_ç«å“8: Optional[UploadFile] = None,
    file_ç«å“9: Optional[UploadFile] = None,
    file_ç«å“10: Optional[UploadFile] = None,
    file_è‡ªèº«ASINåæŸ¥: Optional[UploadFile] = None,
    file_ç«å¯¹ABAçƒ­æœè¯åæŸ¥: Optional[UploadFile] = None,
    file_æ‹“è¯åŸºç¡€è¡¨: Optional[UploadFile] = None,
    folder_files: Optional[List[UploadFile]] = None
):
    """
    H10 ç«å“åˆ†æä¸»å¤„ç†å‡½æ•°
    
    å¤„ç†é€»è¾‘ï¼š
    1. è¯»å–æ‰€æœ‰ä¸Šä¼ çš„æ–‡ä»¶
    2. æ‰§è¡Œä¸‰ä¸ªéƒ¨åˆ†çš„æ ‡è®°é€»è¾‘
    3. è¾“å‡ºå¤„ç†åçš„ H10åæŸ¥æ€»è¡¨
    """
    print("ğŸ” å¼€å§‹ H10 ç«å“åˆ†æå¤„ç†...")
    
    # æ­¥éª¤ 1: æ”¶é›†æ‰€æœ‰æ–‡ä»¶
    files_dict = {}
    file_mapping = {
        "H10åæŸ¥æ€»è¡¨": file_H10åæŸ¥æ€»è¡¨,
        "ç«å“1": file_ç«å“1,
        "ç«å“2": file_ç«å“2,
        "ç«å“3": file_ç«å“3,
        "ç«å“4": file_ç«å“4,
        "ç«å“5": file_ç«å“5,
        "ç«å“6": file_ç«å“6,
        "ç«å“7": file_ç«å“7,
        "ç«å“8": file_ç«å“8,
        "ç«å“9": file_ç«å“9,
        "ç«å“10": file_ç«å“10,
        "è‡ªèº«ASINåæŸ¥": file_è‡ªèº«ASINåæŸ¥,
        "ç«å¯¹ABAçƒ­æœè¯åæŸ¥": file_ç«å¯¹ABAçƒ­æœè¯åæŸ¥,
        "æ‹“è¯åŸºç¡€è¡¨": file_æ‹“è¯åŸºç¡€è¡¨,
    }
    
    # å¤„ç†å•ç‹¬ä¸Šä¼ çš„æ–‡ä»¶
    for key, file in file_mapping.items():
        if file:
            files_dict[key] = file
    
    # å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
    if folder_files:
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶å¤¹ï¼ŒåŒ…å« {len(folder_files)} ä¸ªæ–‡ä»¶")
        # åˆ›å»ºæ–‡ä»¶ååŒ¹é…æ˜ å°„
        name_patterns = {
            "H10åæŸ¥æ€»è¡¨": ["h10", "åæŸ¥æ€»è¡¨", "h10åæŸ¥"],
            "ç«å“1": ["ç«å“1", "ç«å“ 1", "competitor1", "comp1"],
            "ç«å“2": ["ç«å“2", "ç«å“ 2", "competitor2", "comp2"],
            "ç«å“3": ["ç«å“3", "ç«å“ 3", "competitor3", "comp3"],
            "ç«å“4": ["ç«å“4", "ç«å“ 4", "competitor4", "comp4"],
            "ç«å“5": ["ç«å“5", "ç«å“ 5", "competitor5", "comp5"],
            "ç«å“6": ["ç«å“6", "ç«å“ 6", "competitor6", "comp6"],
            "ç«å“7": ["ç«å“7", "ç«å“ 7", "competitor7", "comp7"],
            "ç«å“8": ["ç«å“8", "ç«å“ 8", "competitor8", "comp8"],
            "ç«å“9": ["ç«å“9", "ç«å“ 9", "competitor9", "comp9"],
            "ç«å“10": ["ç«å“10", "ç«å“ 10", "competitor10", "comp10"],
            "è‡ªèº«ASINåæŸ¥": ["è‡ªèº«", "asinåæŸ¥", "è‡ªèº«asin"],
            "ç«å¯¹ABAçƒ­æœè¯åæŸ¥": ["ç«å¯¹", "aba", "çƒ­æœè¯", "å¤šasin"],
            "æ‹“è¯åŸºç¡€è¡¨": ["æ‹“è¯", "åŸºç¡€è¡¨"],
        }
        
        for file in folder_files:
            if not file.filename:
                continue
            filename_lower = file.filename.lower()
            # è‡ªåŠ¨åŒ¹é…æ–‡ä»¶å
            matched = False
            for key, patterns in name_patterns.items():
                if any(pattern in filename_lower for pattern in patterns):
                    if key not in files_dict:  # å¦‚æœè¿˜æ²¡è¢«å•ç‹¬ä¸Šä¼ è¦†ç›–
                        files_dict[key] = file
                        print(f"  âœ… åŒ¹é…æ–‡ä»¶: {file.filename} -> {key}")
                        matched = True
                        break
            if not matched:
                print(f"  âš ï¸ æœªåŒ¹é…æ–‡ä»¶: {file.filename}")
    
    # éªŒè¯å¿…éœ€æ–‡ä»¶
    if "H10åæŸ¥æ€»è¡¨" not in files_dict:
        raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…éœ€æ–‡ä»¶ï¼šH10åæŸ¥æ€»è¡¨")
    
    print(f"ğŸ“Š å·²æ”¶é›† {len(files_dict)} ä¸ªæ–‡ä»¶: {list(files_dict.keys())}")
    
    # æ­¥éª¤ 2: è¯»å–æ‰€æœ‰ Excel æ–‡ä»¶
    dataframes = {}
    for key, file in files_dict.items():
        try:
            content = await file.read()
            # å°è¯•è¯»å– Excel æ–‡ä»¶
            excel_file = pd.ExcelFile(io.BytesIO(content))
            
            # å¦‚æœæœ‰å¤šä¸ªå·¥ä½œè¡¨ï¼Œè¯»å–ç¬¬ä¸€ä¸ªï¼ˆæˆ–æ ¹æ®åç§°åŒ¹é…ï¼‰
            if key == "ç«å¯¹ABAçƒ­æœè¯åæŸ¥":
                # âœ… ä¸¥æ ¼æŒ‰ç…§éœ€æ±‚ï¼šæŸ¥æ‰¾"å¤šasinåæŸ¥æµé‡"å·¥ä½œè¡¨ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
                sheet_name = None
                for sheet in excel_file.sheet_names:
                    if sheet == "å¤šasinåæŸ¥æµé‡" or sheet == "å¤šASINåæŸ¥æµé‡":
                        sheet_name = sheet
                        break
                if not sheet_name:
                    # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                    for sheet in excel_file.sheet_names:
                        if "å¤šasin" in sheet or "å¤šASIN" in sheet:
                            sheet_name = sheet
                            break
                if not sheet_name:
                    print(f"  âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°'å¤šasinåæŸ¥æµé‡'å·¥ä½œè¡¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨")
                    sheet_name = excel_file.sheet_names[0]
                dataframes[key] = pd.read_excel(excel_file, sheet_name=sheet_name)
                print(f"  âœ… {key}: è¯»å–å·¥ä½œè¡¨ '{sheet_name}'")
            else:
                # å…¶ä»–æ–‡ä»¶è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                dataframes[key] = pd.read_excel(excel_file, sheet_name=0)
                print(f"  âœ… {key}: è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨")
        except Exception as e:
            print(f"  âŒ è¯»å– {key} å¤±è´¥: {str(e)}")
            raise HTTPException(status_code=400, detail=f"è¯»å–æ–‡ä»¶ {key} å¤±è´¥: {str(e)}")
    
    # æ­¥éª¤ 3: æ‰§è¡Œå¤„ç†é€»è¾‘
    h10_df = dataframes["H10åæŸ¥æ€»è¡¨"].copy()
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šAN åˆ—æ ‡è®°ï¼ˆA-F ä¼˜å…ˆçº§ï¼‰
    h10_df = process_part1_an_column(h10_df, dataframes)
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šAO åˆ—æ ‡è®°ï¼ˆè¯ç±»å‹åˆ†ç±»ï¼‰
    h10_df = process_part2_ao_column(h10_df, dataframes)
    
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šAP åˆ—æ ‡è®°ï¼ˆæµé‡ç­‰çº§ï¼‰
    h10_df = process_part3_ap_column(h10_df)
    
    # æ­¥éª¤ 4: è¾“å‡ºç»“æœ
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        h10_df.to_excel(writer, sheet_name='H10åæŸ¥æ€»è¡¨', index=False)
    output.seek(0)
    
    print("âœ… H10 å¤„ç†å®Œæˆï¼Œå‡†å¤‡è¿”å›æ–‡ä»¶")
    return output


def process_part1_an_column(h10_df: pd.DataFrame, dataframes: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    ç¬¬ä¸€éƒ¨åˆ†ï¼šAN åˆ—æ ‡è®°ï¼ˆA-F ä¼˜å…ˆçº§ï¼‰
    
    ä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼šF > E > D > C > B > A
    """
    print("ğŸ“ ç¬¬ä¸€éƒ¨åˆ†ï¼šå¤„ç† AN åˆ—æ ‡è®°...")
    
    # æŸ¥æ‰¾å…³é”®è¯è¯ç»„åˆ—ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€åˆ—æˆ–åŒ…å«"å…³é”®è¯"çš„åˆ—ï¼‰
    keyword_col = None
    for col in h10_df.columns:
        if "å…³é”®è¯" in str(col) or "è¯ç»„" in str(col):
            keyword_col = col
            break
    if not keyword_col:
        keyword_col = h10_df.columns[0]  # é»˜è®¤ç¬¬ä¸€åˆ—
    
    # åˆå§‹åŒ– AN åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œåœ¨å…³é”®è¯åˆ—å³ä¾§æ’å…¥ï¼›ä½¿ç”¨ä¸­æ–‡åç§°"å…³é”®è¯ç±»åˆ«"ï¼‰
    an_col_name = "å…³é”®è¯ç±»åˆ«"
    if an_col_name not in h10_df.columns:
        keyword_col_index = h10_df.columns.get_loc(keyword_col)
        h10_df.insert(keyword_col_index + 1, an_col_name, "")
    else:
        # å¦‚æœå·²å­˜åœ¨ï¼Œæ¸…ç©ºæˆ–ä¿æŒ
        h10_df[an_col_name] = h10_df[an_col_name].fillna("")
    
    # æ”¶é›†å„å·¥ä½œè¡¨çš„å…³é”®è¯
    # 1. è‡ªèº«ASINåæŸ¥
    self_asin_keywords = set()
    if "è‡ªèº«ASINåæŸ¥" in dataframes:
        df = dataframes["è‡ªèº«ASINåæŸ¥"]
        # æŸ¥æ‰¾å…³é”®è¯åˆ—
        for col in df.columns:
            if "å…³é”®è¯" in str(col) or "è¯ç»„" in str(col):
                self_asin_keywords = extract_keywords_from_column(df, col)
                break
    
    # 2. ç«å¯¹ABAçƒ­æœè¯åæŸ¥ - å¤šasinåæŸ¥æµé‡å·¥ä½œè¡¨
    aba_keywords = set()
    if "ç«å¯¹ABAçƒ­æœè¯åæŸ¥" in dataframes:
        df = dataframes["ç«å¯¹ABAçƒ­æœè¯åæŸ¥"]
        for col in df.columns:
            if "å…³é”®è¯" in str(col) or "è¯ç»„" in str(col):
                aba_keywords = extract_keywords_from_column(df, col)
                break
    
    # 3. ç«å“1-10
    competitor_keywords_data = {}  # {keyword: {ç«å“: {å¹¿å‘Šæ’å, è‡ªç„¶æ’å}}}
    for i in range(1, 11):
        key = f"ç«å“{i}"
        if key in dataframes:
            df = dataframes[key]
            # æŸ¥æ‰¾å…³é”®è¯åˆ—ã€å¹¿å‘Šæ’ååˆ—ã€è‡ªç„¶æ’ååˆ—
            keyword_col_comp = None
            ad_rank_col = None
            natural_rank_col = None
            
            for col in df.columns:
                col_str = str(col).lower()
                if "å…³é”®è¯" in col_str or "è¯ç»„" in col_str:
                    keyword_col_comp = col
                elif "å¹¿å‘Š" in col_str and ("æ’å" in col_str or "rank" in col_str):
                    ad_rank_col = col
                elif ("è‡ªç„¶" in col_str or "organic" in col_str) and ("æ’å" in col_str or "rank" in col_str):
                    natural_rank_col = col
            
            if keyword_col_comp:
                for idx, row in df.iterrows():
                    keyword = str(row[keyword_col_comp]).strip() if pd.notna(row[keyword_col_comp]) else ""
                    if keyword:
                        if keyword not in competitor_keywords_data:
                            competitor_keywords_data[keyword] = {}
                        
                        ad_rank = None
                        natural_rank = None
                        
                        if ad_rank_col and pd.notna(row[ad_rank_col]):
                            try:
                                ad_rank = float(row[ad_rank_col])
                            except:
                                pass
                        
                        if natural_rank_col and pd.notna(row[natural_rank_col]):
                            try:
                                natural_rank = float(row[natural_rank_col])
                            except:
                                pass
                        
                        competitor_keywords_data[keyword][key] = {
                            "ad_rank": ad_rank,
                            "natural_rank": natural_rank
                        }
    
    # æ ‡è®° AN åˆ—ï¼ˆä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·éœ€æ±‚ï¼Œä¼˜å…ˆçº§ï¼šF > E > D > C > B > Aï¼‰
    print(f"  æ”¶é›†åˆ°çš„å…³é”®è¯æ•°é‡: è‡ªèº«ASIN={len(self_asin_keywords)}, ABA={len(aba_keywords)}, ç«å“={len(competitor_keywords_data)}")
    
    for idx, row in h10_df.iterrows():
        keyword = str(row[keyword_col]).strip() if pd.notna(row[keyword_col]) else ""
        if not keyword:
            h10_df.at[idx, an_col_name] = "A"
            continue
        
        # âœ… ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·éœ€æ±‚çš„ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥
        
        # è§„åˆ™ 1: å¦‚æœå…³é”®è¯åœ¨è‡ªèº«ASINåæŸ¥ä¸­å‡ºç° -> Fï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if keyword in self_asin_keywords:
            h10_df.at[idx, an_col_name] = "F"
            continue
        
        # è§„åˆ™ 2: å¦‚æœå…³é”®è¯åœ¨ç«å¯¹ABAçƒ­æœè¯åæŸ¥ä¸­å‡ºç° -> E
        if keyword in aba_keywords:
            h10_df.at[idx, an_col_name] = "E"
            continue
        
        # è§„åˆ™ 3-6: æ£€æŸ¥ç«å“æ•°æ®
        if keyword in competitor_keywords_data:
            comp_data = competitor_keywords_data[keyword]
            
            # âœ… å‚è€ƒç”¨æˆ·ç‰ˆæœ¬ï¼šæŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥ï¼Œæ‰¾åˆ°æ»¡è¶³æ¡ä»¶åç«‹å³break
            has_d_condition = False
            has_c_condition = False
            has_b_condition = False
            has_a_condition = False
            
            # Dæ¡ä»¶ï¼šå¹¿å‘Šæ’åå’Œè‡ªç„¶æ’åéƒ½<=20ï¼ˆä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³å³å¯ï¼‰
            for comp_name, ranks in comp_data.items():
                ad_rank = ranks.get("ad_rank")
                natural_rank = ranks.get("natural_rank")
                if ad_rank is not None and ad_rank <= 20 and natural_rank is not None and natural_rank <= 20:
                    has_d_condition = True
                    break  # Dä¼˜å…ˆçº§æœ€é«˜ï¼Œæ‰¾åˆ°å°±åœæ­¢
            
            if not has_d_condition:
                # Cæ¡ä»¶ï¼šä»…è‡ªç„¶æ’å<=20ï¼ˆå¹¿å‘Šæ’åéœ€å¤§äº20æˆ–æ— å€¼ï¼Œä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³å³å¯ï¼‰
                for comp_name, ranks in comp_data.items():
                    ad_rank = ranks.get("ad_rank")
                    natural_rank = ranks.get("natural_rank")
                    if natural_rank is not None and natural_rank <= 20 and (ad_rank is None or ad_rank > 20):
                        has_c_condition = True
                        break  # Cä¼˜å…ˆçº§é«˜äºBå’ŒA
                
                if not has_c_condition:
                    # Bæ¡ä»¶ï¼šä»…å¹¿å‘Šæ’å<=20ï¼ˆè‡ªç„¶æ’åéœ€å¤§äº20æˆ–æ— å€¼ï¼Œä»»æ„ä¸€ä¸ªç«å“æ»¡è¶³å³å¯ï¼‰
                    for comp_name, ranks in comp_data.items():
                        ad_rank = ranks.get("ad_rank")
                        natural_rank = ranks.get("natural_rank")
                        if ad_rank is not None and ad_rank <= 20 and (natural_rank is None or natural_rank > 20):
                            has_b_condition = True
                            break  # Bä¼˜å…ˆçº§é«˜äºA
                    
                    if not has_b_condition:
                        # Aæ¡ä»¶ï¼šæ‰€æœ‰ç«å“çš„å¹¿å‘Šæ’åå’Œè‡ªç„¶æ’åéƒ½å¤§äº20æˆ–æ— å€¼ï¼ˆæ‰€æœ‰ç«å“éƒ½è¦æ»¡è¶³ï¼‰
                        all_a_condition = True
                        for comp_name, ranks in comp_data.items():
                            ad_rank = ranks.get("ad_rank")
                            natural_rank = ranks.get("natural_rank")
                            if not ((ad_rank is None or ad_rank > 20) and (natural_rank is None or natural_rank > 20)):
                                all_a_condition = False
                                break
                        if all_a_condition:
                            has_a_condition = True
            
            # æŒ‰ä¼˜å…ˆçº§æ ‡è®°
            if has_d_condition:
                h10_df.at[idx, an_col_name] = "D"
            elif has_c_condition:
                h10_df.at[idx, an_col_name] = "C"
            elif has_b_condition:
                h10_df.at[idx, an_col_name] = "B"
            elif has_a_condition:
                h10_df.at[idx, an_col_name] = "A"
            else:
                # å¦‚æœéƒ½ä¸æ»¡è¶³ï¼Œæ ‡è®°ä¸ºAï¼ˆä¿é™©ï¼‰
                h10_df.at[idx, an_col_name] = "A"
        else:
            # è§„åˆ™ 6: å¦‚æœå…³é”®è¯åœ¨ç«å“1-10ä¸­æœªå‡ºç° -> A
            h10_df.at[idx, an_col_name] = "A"
    
    # ç»Ÿè®¡æ ‡è®°ç»“æœ
    mark_counts = h10_df[an_col_name].value_counts().to_dict()
    print(f"  âœ… AN åˆ—æ ‡è®°å®Œæˆï¼Œå…±å¤„ç† {len(h10_df)} è¡Œï¼Œç»Ÿè®¡: {mark_counts}")
    print(f"  ğŸ“Š ANåˆ—è¯¦ç»†ç»Ÿè®¡: F={mark_counts.get('F', 0)}, E={mark_counts.get('E', 0)}, D={mark_counts.get('D', 0)}, C={mark_counts.get('C', 0)}, B={mark_counts.get('B', 0)}, A={mark_counts.get('A', 0)}")
    return h10_df


def process_part2_ao_column(h10_df: pd.DataFrame, dataframes: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    ç¬¬äºŒéƒ¨åˆ†ï¼šAO åˆ—æ ‡è®°ï¼ˆè¯ç±»å‹åˆ†ç±»ï¼‰
    """
    print("ğŸ“ ç¬¬äºŒéƒ¨åˆ†ï¼šå¤„ç† AO åˆ—æ ‡è®°...")
    
    # åˆå§‹åŒ– AO åˆ—ï¼ˆä½¿ç”¨ä¸­æ–‡åç§°"ç›¸å…³æ€§åˆ†ç±»"ï¼‰
    ao_col_name = "ç›¸å…³æ€§åˆ†ç±»"
    if ao_col_name not in h10_df.columns:
        # æ‰¾åˆ°å…³é”®è¯ç±»åˆ«åˆ—çš„ä½ç½®ï¼Œåœ¨å…¶å³ä¾§æ’å…¥
        an_col_name = "å…³é”®è¯ç±»åˆ«"
        if an_col_name in h10_df.columns:
            an_col_index = h10_df.columns.get_loc(an_col_name)
            h10_df.insert(an_col_index + 1, ao_col_name, "")
        else:
            h10_df.insert(len(h10_df.columns), ao_col_name, "")
    
    # æŸ¥æ‰¾å…³é”®è¯åˆ—
    keyword_col = None
    for col in h10_df.columns:
        if "å…³é”®è¯" in str(col) or "è¯ç»„" in str(col):
            keyword_col = col
            break
    if not keyword_col:
        keyword_col = h10_df.columns[0]
    
    # è¯»å–æ‹“è¯åŸºç¡€è¡¨
    if "æ‹“è¯åŸºç¡€è¡¨" not in dataframes:
        print("  âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æ‹“è¯åŸºç¡€è¡¨ï¼Œè·³è¿‡ AO åˆ—æ ‡è®°")
        return h10_df
    
    tuoci_df = dataframes["æ‹“è¯åŸºç¡€è¡¨"]
    
    # æå–å„åˆ—çš„å€¼ï¼ˆä½œä¸ºå®Œæ•´è¯åŒ¹é…ï¼‰
    col_a_values = []  # Aåˆ—
    col_b_values = []  # Båˆ—
    col_c_values = []  # Cåˆ—
    col_d_values = []  # Dåˆ—
    col_e_values = []  # Eåˆ—
    
    # å‡è®¾åˆ—æŒ‰é¡ºåºï¼šA, B, C, D, E
    if len(tuoci_df.columns) >= 1:
        col_a_values = [str(v).strip() for v in tuoci_df.iloc[:, 0].dropna() if str(v).strip()]
    if len(tuoci_df.columns) >= 2:
        col_b_values = [str(v).strip() for v in tuoci_df.iloc[:, 1].dropna() if str(v).strip()]
    if len(tuoci_df.columns) >= 3:
        col_c_values = [str(v).strip() for v in tuoci_df.iloc[:, 2].dropna() if str(v).strip()]
    if len(tuoci_df.columns) >= 4:
        col_d_values = [str(v).strip() for v in tuoci_df.iloc[:, 3].dropna() if str(v).strip()]
    if len(tuoci_df.columns) >= 5:
        col_e_values = [str(v).strip() for v in tuoci_df.iloc[:, 4].dropna() if str(v).strip()]
    
    print(f"  æ‹“è¯åŸºç¡€è¡¨: Aåˆ—={len(col_a_values)}, Båˆ—={len(col_b_values)}, Cåˆ—={len(col_c_values)}, Dåˆ—={len(col_d_values)}, Eåˆ—={len(col_e_values)}")
    
    # æ ‡è®° AO åˆ—
    for idx, row in h10_df.iterrows():
        keyword = str(row[keyword_col]).strip() if pd.notna(row[keyword_col]) else ""
        if not keyword:
            h10_df.at[idx, ao_col_name] = "ç›¸å…³è¯"
            continue
        
        mark = None
        
        # âœ… å‚è€ƒç”¨æˆ·ç‰ˆæœ¬ï¼šå…ˆè®¡ç®—has_xxxï¼Œç„¶åæŒ‰ä¼˜å…ˆçº§åˆ¤æ–­
        
        # å•è¯çº§åŒ¹é…ï¼šç¡®ä¿åŒ¹é…å€¼ä½œä¸ºç‹¬ç«‹è¯å‡ºç°ï¼ˆä¸è¢«å…¶å®ƒå­—æ¯æ•°å­—è¿åœ¨ä¸€èµ·ï¼‰
        def contains_any(values):
            text = keyword.lower()
            for val in values:
                if not val:
                    continue
                v = str(val).strip().lower()
                if not v:
                    continue
                pattern = rf'(?<![0-9a-zA-Z]){re.escape(v)}(?![0-9a-zA-Z])'
                if re.search(pattern, text):
                    return True
            return False
        
        has_a = contains_any(col_a_values)
        has_b = contains_any(col_b_values)
        has_c = contains_any(col_c_values)
        has_d = contains_any(col_d_values)
        has_e = contains_any(col_e_values)
        
        # âœ… å‚è€ƒç”¨æˆ·ç‰ˆæœ¬çš„ä¼˜å…ˆçº§é¡ºåºåˆ¤æ–­
        # 1. ä¸ç›¸å…³è¯ï¼šåŒ…å«Dåˆ—ä»»æ„å•å…ƒæ ¼çš„å…¨éƒ¨å€¼
        if has_d:
            mark = "ä¸ç›¸å…³è¯"
        # 7. å“ç‰Œè¯ï¼šåŒ…å«Eåˆ—ä»»æ„å•å…ƒæ ¼çš„å…¨éƒ¨å€¼
        elif has_e:
            mark = "å“ç‰Œè¯"
        # 3. aç²¾å‡†å±æ€§ç²¾å‡†è¯ï¼šåŒ…å«Aåˆ—å’ŒBåˆ—ï¼Œä¸”ä¸åŒ…å«Dã€E
        elif has_a and has_b and not has_d and not has_e:
            mark = "aç²¾å‡†å±æ€§ç²¾å‡†è¯"
        # 4. bæ³›å±æ€§ç²¾å‡†è¯ï¼šåŒ…å«Aåˆ—å’ŒCåˆ—ï¼Œä¸”ä¸åŒ…å«Bã€Dã€E
        elif has_a and has_c and not has_b and not has_d and not has_e:
            mark = "bæ³›å±æ€§ç²¾å‡†è¯"
        # 2. å¤§è¯æˆ–æ³›è¯ï¼šåŒ…å«Aåˆ—ï¼Œä¸”ä¸åŒ…å«Bã€Cã€Dã€E
        elif has_a and not has_b and not has_c and not has_d and not has_e:
            mark = "å¤§è¯æˆ–æ³›è¯"
        # 5. ç›¸å…³è¯ï¼šåŒ…å«Bæˆ–Cï¼Œä¸”ä¸åŒ…å«Aã€Dã€E
        elif (has_b or has_c) and not has_a and not has_d and not has_e:
            mark = "ç›¸å…³è¯"
        # 6. ç›¸å…³è¯ï¼šä¸åŒ…å«Aã€Bã€Cã€Dã€E
        elif not has_a and not has_b and not has_c and not has_d and not has_e:
            mark = "ç›¸å…³è¯"
        else:
            mark = "ç›¸å…³è¯"  # é»˜è®¤
        
        h10_df.at[idx, ao_col_name] = mark
    
    # ç»Ÿè®¡æ ‡è®°ç»“æœ
    mark_counts = h10_df[ao_col_name].value_counts().to_dict()
    print(f"  âœ… AO åˆ—æ ‡è®°å®Œæˆï¼Œç»Ÿè®¡: {mark_counts}")
    return h10_df


def process_part3_ap_column(h10_df: pd.DataFrame) -> pd.DataFrame:
    """
    ç¬¬ä¸‰éƒ¨åˆ†ï¼šAP åˆ—æ ‡è®°ï¼ˆæµé‡ç­‰çº§ï¼‰
    
    æ ¹æ®æœç´¢é‡ç´¯è®¡ç™¾åˆ†æ¯”æ ‡è®°ï¼š
    - é«˜æµé‡è¯1: 0-40%
    - ä¸­é«˜æµé‡è¯2: 40-70%
    - ä¸­ä½æµé‡è¯3: 70-90%
    - ä½æµé‡è¯4: 90-100%
    """
    print("ğŸ“ ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¤„ç† AP åˆ—æ ‡è®°...")
    
    # åˆå§‹åŒ– AP åˆ—ï¼ˆä½¿ç”¨ä¸­æ–‡åç§°"æµé‡å¤§å°åˆ†ç±»"ï¼‰
    ap_col_name = "æµé‡å¤§å°åˆ†ç±»"
    if ap_col_name not in h10_df.columns:
        # æ‰¾åˆ°ç›¸å…³æ€§åˆ†ç±»åˆ—çš„ä½ç½®ï¼Œåœ¨å…¶å³ä¾§æ’å…¥
        ao_col_name = "ç›¸å…³æ€§åˆ†ç±»"
        if ao_col_name in h10_df.columns:
            ao_col_index = h10_df.columns.get_loc(ao_col_name)
            h10_df.insert(ao_col_index + 1, ap_col_name, "")
        else:
            h10_df.insert(len(h10_df.columns), ap_col_name, "")
    else:
        # å¦‚æœå·²å­˜åœ¨ï¼Œæ¸…ç©ºæˆ–ä¿æŒ
        h10_df[ap_col_name] = h10_df[ap_col_name].fillna("")
    
    # æŸ¥æ‰¾æœç´¢é‡åˆ—ï¼ˆDåˆ—ï¼Œç´¢å¼•3ï¼‰
    search_volume_col = None
    if len(h10_df.columns) > 3:
        search_volume_col = h10_df.columns[3]  # Dåˆ—
    else:
        # å°è¯•æŸ¥æ‰¾åŒ…å«"æœç´¢é‡"çš„åˆ—
        for col in h10_df.columns:
            if "æœç´¢é‡" in str(col) or "search" in str(col).lower():
                search_volume_col = col
                break
    
    if not search_volume_col:
        print("  âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æœç´¢é‡åˆ—ï¼ˆDåˆ—ï¼‰ï¼Œè·³è¿‡ AP åˆ—æ ‡è®°")
        return h10_df
    
    # æå–æœç´¢é‡å¹¶è½¬æ¢ä¸ºæ•°å€¼
    search_volumes = []
    for idx, row in h10_df.iterrows():
        value = row[search_volume_col]
        if pd.notna(value):
            try:
                vol = float(value)
                search_volumes.append((idx, vol))
            except:
                search_volumes.append((idx, 0))
        else:
            search_volumes.append((idx, 0))
    
    # æŒ‰æœç´¢é‡é™åºæ’åº
    search_volumes.sort(key=lambda x: x[1], reverse=True)
    
    # è®¡ç®—æ€»æœç´¢é‡
    total_volume = sum(vol for _, vol in search_volumes)
    
    if total_volume == 0:
        print("  âš ï¸ è­¦å‘Š: æ€»æœç´¢é‡ä¸º0ï¼Œè·³è¿‡ AP åˆ—æ ‡è®°")
        return h10_df
    
    # è®¡ç®—ç´¯è®¡ç™¾åˆ†æ¯”å¹¶æ ‡è®°
    cumulative_volume = 0
    current_threshold = 0.4  # 40%
    current_mark = "é«˜æµé‡è¯1"
    
    for idx, vol in search_volumes:
        cumulative_volume += vol
        percentage = cumulative_volume / total_volume
        
        if percentage <= 0.4:
            mark = "é«˜æµé‡è¯1"
        elif percentage <= 0.7:
            mark = "ä¸­é«˜æµé‡è¯2"
        elif percentage <= 0.9:
            mark = "ä¸­ä½æµé‡è¯3"
        else:
            mark = "ä½æµé‡è¯4"
        
        h10_df.at[idx, ap_col_name] = mark
    
    print(f"  âœ… AP åˆ—æ ‡è®°å®Œæˆï¼Œæ€»æœç´¢é‡: {total_volume:,.0f}")
    return h10_df

